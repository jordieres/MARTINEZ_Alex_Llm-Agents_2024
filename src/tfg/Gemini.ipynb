{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from vertexai.generative_models import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import requests\n",
    "from typing import Dict\n",
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
    "from pyowm import OWM\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from langchain.tools import tool\n",
    "from habanero import Crossref\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_google_vertexai import VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amartinez/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Agents.WeatherAgent import WeatherAgent\n",
    "from Agents.CrossrefAgent import CrossrefAgent\n",
    "from Agents.ElsevierAgent import ElsevierAgent\n",
    "from Agents.DBAgent import DBAgent\n",
    "from CustomAgentRouter import CustomAgentRouter\n",
    "from HITLAgentRouter import HITLAgentRouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the memory usage\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API keys for various tools\n",
    "\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = \"54b1d6c1af466db06bee6219ddb33f7c\"\n",
    "INFLUXDB_URL = \"https://apiivm78.etsii.upm.es:8086\"\n",
    "INFLUXDB_TOKEN = \"bYNCMsvuiCEoJfPFL5gPgWgDISh79wO4dH9dF_y6cvOKp6uWTRZHtPIwEbRVb2gfFqo3AdygZCQIdbAGBfd31Q==\"\n",
    "INFLUXDB_ORG = \"UPM\"\n",
    "INFLUXDB_BUCKET = \"LoraWAN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"summer-surface-443821-r9\", location=\"europe-southwest1\")\n",
    "\n",
    "model = \"gemini-1.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings ={\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    # temperature (float): The sampling temperature controls the degree of\n",
    "    # randomness in token selection.\n",
    "    \"temperature\": 0.28,\n",
    "    # max_output_tokens (int): The token limit determines the maximum amount of\n",
    "    # text output from one prompt.\n",
    "    \"max_output_tokens\": 1000,\n",
    "    # top_p (float): Tokens are selected from most probable to least until\n",
    "    # the sum of their probabilities equals the top-p value.\n",
    "    \"top_p\": 0.95,\n",
    "    # top_k (int): The next token is selected from among the top-k most\n",
    "    # probable tokens.\n",
    "    \"top_k\": 40,\n",
    "    # safety_settings (Dict[HarmCategory, HarmBlockThreshold]): The safety\n",
    "    # settings to use for generating content.\n",
    "    \"safety_settings\": safety_settings,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_814638/2280263513.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Define memory buffer\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the answer structure\n",
    "\n",
    "PREFIX = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "FORMAT_INSTRUCTIONS = \"\"\"To use a tool, please use the following format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "```\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "```\"\"\"\n",
    "SUFFIX = \"\"\"Begin!\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "\n",
    "template = \"\\n\\n\".join([PREFIX, \"{tools}\", FORMAT_INSTRUCTIONS, SUFFIX])\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\", \"chat_history\", \"tools\", \"tool_names\", \"agent_scratchpad\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenWeatherMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWeatherMap configuration\n",
    "API_KEY = \"54b1d6c1af466db06bee6219ddb33f7c\"\n",
    "owm = OWM(API_KEY)\n",
    "mgr = owm.weather_manager()\n",
    "\n",
    "# Define a function to obtain climate\n",
    "def get_weather(location: str) -> str:\n",
    "    try:\n",
    "        observation = mgr.weather_at_place(location)\n",
    "        weather = observation.weather\n",
    "        return (\n",
    "            f\"The current weather in {location} is {weather.status} with a temperature of \"\n",
    "            f\"{weather.temperature('celsius')['temp']}¬∞C.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Failed to fetch weather data for {location}: {str(e)}\"\n",
    "\n",
    "# Create compatible tool\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    func=get_weather,\n",
    "    description=\"Fetch the current weather for a specified location.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "from langchain.tools import Tool\n",
    "import json\n",
    "\n",
    "# Client configuration\n",
    "INFLUXDB_URL = \"https://apiivm78.etsii.upm.es:8086\"\n",
    "INFLUXDB_TOKEN = \"bYNCMsvuiCEoJfPFL5gPgWgDISh79wO4dH9dF_y6cvOKp6uWTRZHtPIwEbRVb2gfFqo3AdygZCQIdbAGBfd31Q==\"\n",
    "INFLUXDB_ORG = \"UPM\"\n",
    "INFLUXDB_BUCKET = \"LoraWAN\"\n",
    "\n",
    "client = InfluxDBClient(\n",
    "    url=INFLUXDB_URL,\n",
    "    token=INFLUXDB_TOKEN,\n",
    "    org=INFLUXDB_ORG\n",
    ")\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Supported metrics\n",
    "VALID_METRICS = {\"temperature\", \"humidity\", \"light\", \"human_presence\"}\n",
    "VALID_AGGREGATIONS = {\"mean\", \"max\", \"min\", \"sum\"}\n",
    "\n",
    "# Function to construct Flux query dynamically\n",
    "def construct_flux_query(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a Flux query based on extracted parameters.\n",
    "    Args:\n",
    "        params (dict): A dictionary containing 'metric', 'time_range', and 'aggregation'.\n",
    "    Returns:\n",
    "        str: A formatted Flux query.\n",
    "    \"\"\"\n",
    "    measurement = \"sensor_data\"  # Hardcoded since all fields are inside 'sensor_data'\n",
    "    field = params.get(\"metric\", \"humidity\")  # Default to humidity if not specified\n",
    "    time_range = params.get(\"time_range\", \"24h\")\n",
    "    aggregation = params.get(\"aggregation\", \"mean\")\n",
    "\n",
    "    flux_query = f\"\"\"\n",
    "    from(bucket: \"LoraWAN\")\n",
    "      |> range(start: -{time_range})\n",
    "      |> filter(fn: (r) => r[\"_measurement\"] == \"{measurement}\")\n",
    "      |> filter(fn: (r) => r[\"_field\"] == \"{field}\")\n",
    "      |> aggregateWindow(every: 1h, fn: {aggregation}, createEmpty: false)\n",
    "      |> yield(name: \"result\")\n",
    "    \"\"\"\n",
    "    return flux_query\n",
    "\n",
    "\n",
    "# Function to execute the generated Flux query\n",
    "def query_influxdb(params: dict):\n",
    "    \"\"\"\n",
    "    Constructs and executes a Flux query on InfluxDB.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Dictionary containing query parameters (metric, time_range, aggregation).\n",
    "    \n",
    "    Returns:\n",
    "        str: Query results or an error message.\n",
    "    \"\"\"\n",
    "    flux_query = construct_flux_query(params)\n",
    "\n",
    "    try:\n",
    "        print(f\"üìä Extracted Parameters: {params}\")\n",
    "        print(f\"üî• Executing Flux Query:\\n{flux_query}\") \n",
    "\n",
    "        result = query_api.query(org=INFLUXDB_ORG, query=flux_query)\n",
    "        results = []\n",
    "\n",
    "        for table in result:\n",
    "            for record in table.records:\n",
    "                results.append(f\"Time: {record.get_time()}, Value: {record.get_value()}\")\n",
    "\n",
    "        return \"\\n\".join(results) if results else \"‚ö†Ô∏è No data found in the database. Verify if data exists for this time range.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error querying InfluxDB: {str(e)}\"\n",
    "\n",
    "\n",
    "# Create compatible tool\n",
    "influx_tool = Tool(\n",
    "    name=\"InfluxDB Query Tool\",\n",
    "    func=query_influxdb,\n",
    "    description=\"Fetches sensor data from InfluxDB. Requires parameters like metric (e.g., humidity, temperature), time_range (e.g., 24h), and aggregation (e.g., mean).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Extracted Parameters: {'metric': 'humidity', 'time_range': '3d', 'aggregation': 'mean'}\n",
      "üî• Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -3d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"humidity\")\n",
      "      |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "Time: 2025-03-21 19:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 20:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 21:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 22:00:00+00:00, Value: 62.2\n",
      "Time: 2025-03-21 23:00:00+00:00, Value: 62.5\n",
      "Time: 2025-03-22 00:00:00+00:00, Value: 62.8\n",
      "Time: 2025-03-22 01:00:00+00:00, Value: 62.36363636363637\n",
      "Time: 2025-03-22 02:00:00+00:00, Value: 62.125\n",
      "Time: 2025-03-22 03:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 04:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 05:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 06:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 07:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 08:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 09:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 10:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 11:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 12:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 13:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 14:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 15:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 16:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 61.0\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 60.75\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 60.0\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 59.9\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 59.72727272727273\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 60.0\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 60.0\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Retrieve the average humidity from the last 24 hours.\"\n",
    "\n",
    "# Ejemplo de c√≥mo deber√≠a estructurar la salida el agente\n",
    "extracted_parameters = {\n",
    "    \"metric\": \"humidity\",\n",
    "    \"time_range\": \"3d\",\n",
    "    \"aggregation\": \"mean\"\n",
    "}\n",
    "\n",
    "# Ahora la herramienta se encarga del resto\n",
    "result = query_influxdb(extracted_parameters)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"metric\": \"temperature\", \"time_range\": \"24h\", \"aggregation\": \"mean\"}\n",
    "print(execute_flux_query(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "from langchain.tools import Tool\n",
    "import re\n",
    "\n",
    "# Client configuration\n",
    "INFLUXDB_URL = \"https://apiivm78.etsii.upm.es:8086\"\n",
    "INFLUXDB_TOKEN = \"bYNCMsvuiCEoJfPFL5gPgWgDISh79wO4dH9dF_y6cvOKp6uWTRZHtPIwEbRVb2gfFqo3AdygZCQIdbAGBfd31Q==\"\n",
    "INFLUXDB_ORG = \"UPM\"\n",
    "INFLUXDB_BUCKET = \"LoraWAN\"\n",
    "\n",
    "client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Supported parameters\n",
    "VALID_METRICS = {\"temperature\", \"humidity\", \"light\", \"motion\", \"vdd\"}\n",
    "VALID_AGGREGATIONS = {\"mean\", \"max\", \"min\", \"sum\"}\n",
    "\n",
    "# Function to construct Flux query dynamically\n",
    "def construct_flux_query(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a Flux query based on extracted parameters.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): A dictionary containing 'metric', 'time_range', and 'aggregation'.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted Flux query.\n",
    "    \"\"\"\n",
    "    measurement = \"sensor_data\"  # Default measurement\n",
    "    field = params.get(\"metric\", \"humidity\")  # Default metric\n",
    "    time_range = params.get(\"time_range\", \"24h\")  # Default to 24h if missing\n",
    "    aggregation = params.get(\"aggregation\", \"mean\")  # Default to mean\n",
    "\n",
    "    # Validate metric\n",
    "    if field not in VALID_METRICS:\n",
    "        raise ValueError(f\"‚ùå Invalid metric '{field}'. Available metrics: {', '.join(VALID_METRICS)}\")\n",
    "\n",
    "    # Validate aggregation function\n",
    "    if aggregation not in VALID_AGGREGATIONS:\n",
    "        raise ValueError(f\"‚ùå Invalid aggregation '{aggregation}'. Available functions: {', '.join(VALID_AGGREGATIONS)}\")\n",
    "\n",
    "    # Construct Flux query\n",
    "    flux_query = f\"\"\"\n",
    "    from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "      |> range(start: -{time_range})\n",
    "      |> filter(fn: (r) => r[\"_measurement\"] == \"{measurement}\")\n",
    "      |> filter(fn: (r) => r[\"_field\"] == \"{field}\")\n",
    "      |> aggregateWindow(every: 1h, fn: {aggregation}, createEmpty: false)\n",
    "      |> yield(name: \"result\")\n",
    "    \"\"\"\n",
    "    return flux_query\n",
    "\n",
    "# Function to execute a Flux query\n",
    "def query_influxdb(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs and executes a Flux query on InfluxDB.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Dictionary containing query parameters (metric, time_range, aggregation).\n",
    "    \n",
    "    Returns:\n",
    "        str: Query results or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        flux_query = construct_flux_query(params)\n",
    "\n",
    "        print(f\"üìä Extracted Parameters: {params}\")\n",
    "        print(f\"üî• Executing Flux Query:\\n{flux_query}\") \n",
    "\n",
    "        result = query_api.query(org=INFLUXDB_ORG, query=flux_query)\n",
    "        results = []\n",
    "\n",
    "        for table in result:\n",
    "            for record in table.records:\n",
    "                results.append(f\"Time: {record.get_time()}, Value: {record.get_value()}\")\n",
    "\n",
    "        return \"\\n\".join(results) if results else \"‚ö†Ô∏è No data found in the database. Verify if data exists for this time range.\"\n",
    "    \n",
    "    except ValueError as ve:\n",
    "        return str(ve)  # Return validation error messages\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error querying InfluxDB: {str(e)}\"\n",
    "\n",
    "# Function to extract the time range from a query\n",
    "def extract_time_range(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the time range from a user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The input query from the user.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted time range for InfluxDB (e.g., \"24h\", \"7d\", \"30d\").\n",
    "    \"\"\"\n",
    "    time_patterns = {\n",
    "        r\"(\\d+)\\s*(minute|minutes|min)\": \"m\",\n",
    "        r\"(\\d+)\\s*(hour|hours|h)\": \"h\",\n",
    "        r\"(\\d+)\\s*(day|days|d)\": \"d\",\n",
    "        r\"(\\d+)\\s*(week|weeks|w)\": \"w\",\n",
    "        r\"(\\d+)\\s*(month|months|mo)\": \"d\",  # Approximate: 1 month = 30 days\n",
    "        r\"(\\d+)\\s*(year|years|y)\": \"d\"  # Approximate: 1 year = 365 days\n",
    "    }\n",
    "\n",
    "    detected_range = \"24h\"  # Default if no time range is found\n",
    "\n",
    "    for pattern, unit in time_patterns.items():\n",
    "        match = re.search(pattern, user_query, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = int(match.group(1))  # Extract numeric value\n",
    "            if \"month\" in pattern:\n",
    "                value *= 30  # Convert months to days\n",
    "            elif \"year\" in pattern:\n",
    "                value *= 365  # Convert years to days\n",
    "            detected_range = f\"{value}{unit}\"\n",
    "            break\n",
    "\n",
    "    return detected_range\n",
    "\n",
    "# LangChain compatible tool\n",
    "influx_tool = Tool(\n",
    "    name=\"InfluxDB Query Tool\",\n",
    "    func=query_influxdb,\n",
    "    description=\"Fetches sensor data from InfluxDB. Requires parameters like metric (e.g., humidity, temperature), time_range (e.g., 24h), and aggregation (e.g., mean).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Extracted Parameters: {'metric': 'temperature', 'time_range': '3d', 'aggregation': 'mean'}\n",
      "üî• Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -3d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"temperature\")\n",
      "      |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "Time: 2025-03-21 19:00:00+00:00, Value: 15.900000000000002\n",
      "Time: 2025-03-21 20:00:00+00:00, Value: 15.83846153846154\n",
      "Time: 2025-03-21 21:00:00+00:00, Value: 15.799999999999999\n",
      "Time: 2025-03-21 22:00:00+00:00, Value: 15.749999999999996\n",
      "Time: 2025-03-21 23:00:00+00:00, Value: 15.716666666666663\n",
      "Time: 2025-03-22 00:00:00+00:00, Value: 15.66\n",
      "Time: 2025-03-22 01:00:00+00:00, Value: 15.599999999999996\n",
      "Time: 2025-03-22 02:00:00+00:00, Value: 15.574999999999998\n",
      "Time: 2025-03-22 03:00:00+00:00, Value: 15.51\n",
      "Time: 2025-03-22 04:00:00+00:00, Value: 15.470000000000002\n",
      "Time: 2025-03-22 05:00:00+00:00, Value: 15.400000000000004\n",
      "Time: 2025-03-22 06:00:00+00:00, Value: 15.377777777777778\n",
      "Time: 2025-03-22 07:00:00+00:00, Value: 15.299999999999999\n",
      "Time: 2025-03-22 08:00:00+00:00, Value: 15.300000000000002\n",
      "Time: 2025-03-22 09:00:00+00:00, Value: 15.254545454545452\n",
      "Time: 2025-03-22 10:00:00+00:00, Value: 15.199999999999998\n",
      "Time: 2025-03-22 11:00:00+00:00, Value: 15.169999999999998\n",
      "Time: 2025-03-22 12:00:00+00:00, Value: 15.111111111111107\n",
      "Time: 2025-03-22 13:00:00+00:00, Value: 15.099999999999998\n",
      "Time: 2025-03-22 14:00:00+00:00, Value: 15.054545454545455\n",
      "Time: 2025-03-22 15:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 16:00:00+00:00, Value: 14.98\n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 14.954545454545457\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 14.91666666666667\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 14.912500000000001\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 14.863636363636367\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 14.774999999999997\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 14.733333333333333\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 14.688888888888888\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 14.633333333333331\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 14.618181818181817\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 14.599999999999998\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 14.559999999999999\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 14.430000000000001\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 14.400000000000002\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 14.363636363636367\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 14.350000000000003\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 14.350000000000001\n"
     ]
    }
   ],
   "source": [
    "query_params = {\n",
    "    \"metric\": \"temperature\",\n",
    "    \"time_range\": extract_time_range(\"Show me the mean temperature last 3 days.\"),\n",
    "    \"aggregation\": \"mean\"\n",
    "}\n",
    "\n",
    "result = query_influxdb(query_params)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elsevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the content of a specific article using Elsevier's APIs based on its title.\n",
    "\n",
    "    Args:\n",
    "    - title (str): The title of the article to search for.\n",
    "\n",
    "    Returns:\n",
    "    - str: The content or abstract of the specified article.\n",
    "    \"\"\"\n",
    "\n",
    "    API_KEY = \"87ab69edd16f0cdb92e611b99b8f4ee6\"\n",
    "    BASE_SEARCH_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    BASE_ARTICLE_URL = \"https://api.elsevier.com/content/article/doi\"\n",
    "    BASE_ARTICLE_EID_URL = \"https://api.elsevier.com/content/article/eid\"\n",
    "\n",
    "    # 1. Search article by title\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-ELS-APIKey\": API_KEY,\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": f\"TITLE({title})\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "    \n",
    "    search_response = requests.get(BASE_SEARCH_URL, headers=headers, params=params)\n",
    "\n",
    "    if search_response.status_code != 200:\n",
    "        return f\"Error: No se pudo buscar el art√≠culo. C√≥digo de estado: {search_response.status_code}, Error: {search_response.text}\"\n",
    "\n",
    "    search_data = search_response.json()\n",
    "    entries = search_data.get(\"search-results\", {}).get(\"entry\", [])\n",
    "\n",
    "    if not entries:\n",
    "        return f\"No se encontr√≥ ning√∫n art√≠culo con el t√≠tulo '{title}'.\"\n",
    "\n",
    "    # 2. Fetch article identifier\n",
    "    article_entry = entries[0]\n",
    "    print(\"Art√≠culo encontrado:\", article_entry)  # Debugging info\n",
    "\n",
    "    doi = article_entry.get(\"prism:doi\")\n",
    "    eid = article_entry.get(\"eid\")\n",
    "\n",
    "    if not doi and not eid:\n",
    "        return f\"No se encontr√≥ DOI ni EID para el art√≠culo con el t√≠tulo '{title}'. No se puede recuperar el contenido.\"\n",
    "\n",
    "    # 3. Fetch article content using DOI\n",
    "    if doi:\n",
    "        article_url = f\"{BASE_ARTICLE_URL}/{doi}\"\n",
    "    else:\n",
    "        article_url = f\"{BASE_ARTICLE_EID_URL}/{eid}\"\n",
    "\n",
    "    print(f\"Intentando recuperar el art√≠culo desde: {article_url}\")  # Debugging info\n",
    "\n",
    "    article_response = requests.get(article_url, headers=headers)\n",
    "\n",
    "    if article_response.status_code != 200:\n",
    "        return f\"Error: No se pudo recuperar el contenido del art√≠culo. C√≥digo de estado: {article_response.status_code}, Error: {article_response.text}\"\n",
    "\n",
    "    article_data = article_response.json()\n",
    "\n",
    "    abstract = article_data.get(\"full-text-retrieval-response\", {}).get(\"coredata\", {}).get(\"dc:description\", \"No abstract found\")\n",
    "\n",
    "    return f\"Contenido del art√≠culo '{title}':\\n\\n{abstract}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Taking the next step with generative artificial intelligence: The transformative role of multimodal large language models in science education\"\n",
    "content = get_article_content(title)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossref(subject: str) -> Dict:\n",
    "    limit = 5\n",
    "    results = {}\n",
    "    cr = Crossref()\n",
    "    try:\n",
    "        result = cr.works(query=subject, limit=limit)\n",
    "        for i in range(0, limit-1):\n",
    "            title = result['message']['items'][i].get('title', ['No Title'])[0]\n",
    "            abstract = result['message']['items'][i].get('abstract', 'No abstract available')\n",
    "            results[title] = abstract\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "crossref_tool = Tool(\n",
    "    name=\"Crossref article search\",\n",
    "    func=crossref,\n",
    "    description=\"Use this to fetch article titles and abstracts based on a given subject.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Crossref()\n",
    "x = cr.works(query = \"ecology\")\n",
    "x['message']['items'][4]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = crossref(subject=\"computing\")\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agent = WeatherAgent()\n",
    "\n",
    "print(\"Registered tools:\", weather_agent.agent.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agent.run(\"Is it raining in Manchester?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered tools: [Tool(name='InfluxDB Query Tool', description='Fetches sensor data from InfluxDB. Requires parameters like metric (e.g., humidity, temperature), time_range (e.g., 24h), and aggregation (e.g., mean).', func=<function query_influxdb at 0x796e0e1c0360>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/home/amartinez/TFG/MARTINEZ_Alex_Llm-Agents_2024/src/tfg/Agents/BaseAgent.py:44: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  self.agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "DB_agent = DBAgent()\n",
    "\n",
    "print(\"Registered tools:\", DB_agent.agent.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Query being processed by agent:\n",
      "Show me the mean temperature last 3 days.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/home/amartinez/TFG/MARTINEZ_Alex_Llm-Agents_2024/src/tfg/Agents/BaseAgent.py:63: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.agent.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I need to fetch the average temperature data from the last 3 days.\n",
      "Action: InfluxDB Query Tool\n",
      "Action Input: {'metric': 'temperature', 'time_range': '3d', 'aggregation': 'mean'}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m‚ùå Error querying InfluxDB: 'str' object has no attribute 'get'\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mQuestion: Show me the mean temperature last 3 days.\n",
      "Thought: I need to fetch the average temperature data from the last 3 days.\n",
      "Action: InfluxDB Query Tool\n",
      "Action Input: {'metric': 'temperature', 'time_range': '3d', 'aggregation': 'mean'}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m‚ùå Error querying InfluxDB: 'str' object has no attribute 'get'\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mfrom(bucket: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoraWAN\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  |> range(start: 2024-12-01T00:00:00Z, stop: 2025-01-01T00:00:00Z)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  |> yield(name: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mDB_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShow me the mean temperature last 3 days.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sdb1/home/amartinez/TFG/MARTINEZ_Alex_Llm-Agents_2024/src/tfg/Agents/BaseAgent.py:63\u001b[0m, in \u001b[0;36mBaseAgent.run\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Query being processed by agent:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:606\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    607\u001b[0m         _output_key\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    612\u001b[0m         _output_key\n\u001b[1;32m    613\u001b[0m     ]\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1624\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1624\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1633\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1634\u001b[0m         )\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1330\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1330\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1340\u001b[0m     )\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1358\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1358\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:804\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    803\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 804\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:759\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    753\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    757\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    758\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:962\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    949\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    950\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    960\u001b[0m         )\n\u001b[1;32m    961\u001b[0m     ]\n\u001b[0;32m--> 962\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:796\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    795\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 796\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    797\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:783\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    775\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    780\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 783\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    791\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_google_vertexai/llms.py:252\u001b[0m, in \u001b[0;36mVertexAI._generate\u001b[0;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([generation])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_gemini_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_gemini_model:\n\u001b[1;32m    261\u001b[0m         usage_metadata \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mto_dict()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_google_vertexai/llms.py:71\u001b[0m, in \u001b[0;36m_completion_with_retry\u001b[0;34m(llm, prompt, stream, is_gemini, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpredict(prompt[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m telemetry\u001b[38;5;241m.\u001b[39mtool_context_manager(llm\u001b[38;5;241m.\u001b[39m_user_agent):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tenacity/__init__.py:485\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    484\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/TFG/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "from(bucket: \"LoraWAN\")\n",
    "  |> range(start: 2024-12-01T00:00:00Z, stop: 2025-01-01T00:00:00Z)\n",
    "  |> filter(fn: (r) => r._measurement == \"sensor_data\")\n",
    "  |> filter(fn: (r) => r._field == \"humidity\" or r._field == \"light\")\n",
    "  |> aggregateWindow(every: 1d, fn: mean, createEmpty: false)\n",
    "  |> yield(name: \"mean\")\n",
    "\"\"\"\n",
    "\n",
    "DB_agent.run(\"Show me the mean temperature last 4 days.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elsevier Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsevier_agent = ElsevierAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsevier_agent.run(\"Search some articles about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossref Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_agent = CrossrefAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_agent.run(\"I want to know more about quantum computing, can you help me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents dictionary\n",
    "agents = {\n",
    "    \"weather\": weather_agent,\n",
    "    \"article title\": crossref_agent,\n",
    "    \"article content\": elsevier_agent,\n",
    "    \"generic\": weather_agent,  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the router\n",
    "router = CustomAgentRouter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is the weather like in Madrid?\",  # Should go to WeatherAgent\n",
    "    \"Search for articles about machine learning.\",  # Should go to CrossrefAgent\n",
    "    \"I need the content of an article titled 'Deep Learning Basics'.\"  # Should go to ElsevierAgent\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline(\"What is the weather like in Madrid?\")\n",
    "print(\"Predictions output:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = router.run( \"What is the weather like in Madrid?\")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in inputs:\n",
    "    print(f\"Input: {input_text}\")\n",
    "    response = router.run(input_text)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph as lg\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Dict, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n",
    "\n",
    "# Placeholder functions for agents\n",
    "def weather_agent(input_text: str) -> str:\n",
    "    return f\"Weather Agent processed: {input_text}\"\n",
    "\n",
    "def db_agent(input_text: str) -> str:\n",
    "    return f\"Database Agent processed: {input_text}\"\n",
    "\n",
    "def article_agent(input_text: str) -> str:\n",
    "    return f\"Article Agent processed: {input_text}\"\n",
    "\n",
    "def classify_query(input_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Simple classification logic to determine the category of the query.\n",
    "    \"\"\"\n",
    "    if any(word in input_text.lower() for word in [\"weather\", \"forecast\", \"temperature\"]):\n",
    "        return {\"route\": \"weather\"}\n",
    "    elif any(word in input_text.lower() for word in [\"database\", \"query\", \"influx\"]):\n",
    "        return {\"route\": \"database\"}\n",
    "    elif any(word in input_text.lower() for word in [\"article\", \"research\", \"journal\"]):\n",
    "        return {\"route\": \"article\"}\n",
    "    else:\n",
    "        return {\"route\": \"unknown\"}\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "tools=[weather_tool]\n",
    "\n",
    "model_name= \"gemini-1.5-flash\"\n",
    "project=\"summer-surface-443821-r9\"\n",
    "location=\"europe-southwest1\"\n",
    "llm = ChatAnthropicVertex(model_name=model_name, project=project, location=location)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Define graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Define nodes\n",
    "#workflow.add_node(\"classify\", classify_query)\n",
    "#workflow.add_node(\"weather\", weather_agent)\n",
    "#workflow.add_node(\"database\", db_agent)\n",
    "#workflow.add_node(\"article\", article_agent)\n",
    "\n",
    "# Define edges\n",
    "# workflow.set_entry_point(\"classify\")\n",
    "#workflow.add_edge(\"classify\", \"weather\", condition=lambda x: x[\"route\"] == \"weather\")\n",
    "#workflow.add_edge(\"classify\", \"database\", condition=lambda x: x[\"route\"] == \"database\")\n",
    "#workflow.add_edge(\"classify\", \"article\", condition=lambda x: x[\"route\"] == \"article\")\n",
    "\n",
    "# Compile graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "# Example usage\n",
    "#query = \"What is the temperature in Madrid?\"\n",
    "#response = graph.invoke(query)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
