{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from vertexai.generative_models import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import requests\n",
    "from typing import Dict\n",
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
    "from pyowm import OWM\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from langchain.tools import tool\n",
    "from habanero import Crossref\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_google_vertexai import VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amartinez/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Agents.WeatherAgent import WeatherAgent\n",
    "from Agents.CrossrefAgent import CrossrefAgent\n",
    "from Agents.ElsevierAgent import ElsevierAgent\n",
    "from Agents.DBAgent import DBAgent\n",
    "from CustomAgentRouter import CustomAgentRouter\n",
    "from HITLAgentRouter import HITLAgentRouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the memory usage\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API keys for various tools\n",
    "\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = \"54b1d6c1af466db06bee6219ddb33f7c\"\n",
    "INFLUXDB_URL = \"https://apiivm78.etsii.upm.es:8086\"\n",
    "INFLUXDB_TOKEN = \"bYNCMsvuiCEoJfPFL5gPgWgDISh79wO4dH9dF_y6cvOKp6uWTRZHtPIwEbRVb2gfFqo3AdygZCQIdbAGBfd31Q==\"\n",
    "INFLUXDB_ORG = \"UPM\"\n",
    "INFLUXDB_BUCKET = \"LoraWAN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"summer-surface-443821-r9\", location=\"europe-southwest1\")\n",
    "\n",
    "model = \"gemini-1.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings ={\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    # temperature (float): The sampling temperature controls the degree of\n",
    "    # randomness in token selection.\n",
    "    \"temperature\": 0.28,\n",
    "    # max_output_tokens (int): The token limit determines the maximum amount of\n",
    "    # text output from one prompt.\n",
    "    \"max_output_tokens\": 1000,\n",
    "    # top_p (float): Tokens are selected from most probable to least until\n",
    "    # the sum of their probabilities equals the top-p value.\n",
    "    \"top_p\": 0.95,\n",
    "    # top_k (int): The next token is selected from among the top-k most\n",
    "    # probable tokens.\n",
    "    \"top_k\": 40,\n",
    "    # safety_settings (Dict[HarmCategory, HarmBlockThreshold]): The safety\n",
    "    # settings to use for generating content.\n",
    "    \"safety_settings\": safety_settings,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3930683/2280263513.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Define memory buffer\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the answer structure\n",
    "\n",
    "PREFIX = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "FORMAT_INSTRUCTIONS = \"\"\"To use a tool, please use the following format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "```\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "```\"\"\"\n",
    "SUFFIX = \"\"\"Begin!\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "\n",
    "template = \"\\n\\n\".join([PREFIX, \"{tools}\", FORMAT_INSTRUCTIONS, SUFFIX])\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\", \"chat_history\", \"tools\", \"tool_names\", \"agent_scratchpad\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenWeatherMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWeatherMap configuration\n",
    "API_KEY = \"54b1d6c1af466db06bee6219ddb33f7c\"\n",
    "owm = OWM(API_KEY)\n",
    "mgr = owm.weather_manager()\n",
    "\n",
    "# Define a function to obtain climate\n",
    "def get_weather(location: str) -> str:\n",
    "    try:\n",
    "        observation = mgr.weather_at_place(location)\n",
    "        weather = observation.weather\n",
    "        return (\n",
    "            f\"The current weather in {location} is {weather.status} with a temperature of \"\n",
    "            f\"{weather.temperature('celsius')['temp']}Â°C.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Failed to fetch weather data for {location}: {str(e)}\"\n",
    "\n",
    "# Create compatible tool\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    func=get_weather,\n",
    "    description=\"Fetch the current weather for a specified location.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "from langchain.tools import Tool\n",
    "import re\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from langchain.tools.base import StructuredTool\n",
    "\n",
    "# Client configuration\n",
    "INFLUXDB_URL = \"https://apiivm78.etsii.upm.es:8086\"\n",
    "INFLUXDB_TOKEN = \"bYNCMsvuiCEoJfPFL5gPgWgDISh79wO4dH9dF_y6cvOKp6uWTRZHtPIwEbRVb2gfFqo3AdygZCQIdbAGBfd31Q==\"\n",
    "INFLUXDB_ORG = \"UPM\"\n",
    "INFLUXDB_BUCKET = \"LoraWAN\"\n",
    "\n",
    "client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Supported parameters\n",
    "VALID_METRICS = {\"temperature\", \"humidity\", \"light\", \"motion\", \"vdd\"}\n",
    "VALID_AGGREGATIONS = {\"mean\", \"max\", \"min\", \"sum\"}\n",
    "\n",
    "# Define expected input schema for the tool using Pydantic\n",
    "class InfluxDBQueryInput(BaseModel):\n",
    "    \"\"\"\n",
    "    Defines the input parameters required for querying InfluxDB.\n",
    "    \"\"\"\n",
    "    metric: str          # Sensor metric to query (e.g., temperature, humidity)\n",
    "    time_range: str      # Time range for the query (e.g., 24h, 7d)\n",
    "    aggregation: str     # Aggregation function (e.g., mean, max)\n",
    "\n",
    "# Function to construct Flux query dynamically\n",
    "def construct_flux_query(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a Flux query based on extracted parameters.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): A dictionary containing 'metric', 'time_range', and 'aggregation'.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted Flux query.\n",
    "    \"\"\"\n",
    "    measurement = \"sensor_data\"  # Default measurement\n",
    "    field = params.get(\"metric\", \"humidity\")  # Default metric\n",
    "    time_range = params.get(\"time_range\", \"24h\")  # Default to 24h if missing\n",
    "    aggregation = params.get(\"aggregation\", \"mean\")  # Default to mean\n",
    "\n",
    "    # Validate metric\n",
    "    if field not in VALID_METRICS:\n",
    "        raise ValueError(f\"âŒ Invalid metric '{field}'. Available metrics: {', '.join(VALID_METRICS)}\")\n",
    "\n",
    "    # Validate aggregation function\n",
    "    if aggregation not in VALID_AGGREGATIONS:\n",
    "        raise ValueError(f\"âŒ Invalid aggregation '{aggregation}'. Available functions: {', '.join(VALID_AGGREGATIONS)}\")\n",
    "\n",
    "    # Construct Flux query\n",
    "    flux_query = f\"\"\"\n",
    "    from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "      |> range(start: -{time_range})\n",
    "      |> filter(fn: (r) => r[\"_measurement\"] == \"{measurement}\")\n",
    "      |> filter(fn: (r) => r[\"_field\"] == \"{field}\")\n",
    "      |> aggregateWindow(every: 1h, fn: {aggregation}, createEmpty: false)\n",
    "      |> yield(name: \"result\")\n",
    "    \"\"\"\n",
    "    return flux_query\n",
    "\n",
    "# Function that receives individual arguments (required by StructuredTool)\n",
    "def query_influxdb(metric: str, time_range: str, aggregation: str) -> str:\n",
    "    \"\"\"\n",
    "    StructuredTool-compatible function to query InfluxDB using individual parameters.\n",
    "\n",
    "    Args:\n",
    "        metric (str): Sensor metric to query (e.g., \"temperature\", \"humidity\").\n",
    "        time_range (str): Time range for the query (e.g., \"24h\", \"7d\").\n",
    "        aggregation (str): Aggregation function to apply (e.g., \"mean\", \"max\").\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted result or error message.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"metric\": metric,\n",
    "        \"time_range\": time_range,\n",
    "        \"aggregation\": aggregation\n",
    "    }\n",
    "    return _query_influxdb_internal(params)\n",
    "\n",
    "# Internal function to perform the actual query logic\n",
    "def _query_influxdb_internal(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs and executes a Flux query on InfluxDB from parameter dictionary.\n",
    "\n",
    "    Args:\n",
    "        params (dict): Dictionary containing 'metric', 'time_range', and 'aggregation'.\n",
    "\n",
    "    Returns:\n",
    "        str: Query results or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build the Flux query dynamically from parameters\n",
    "        flux_query = construct_flux_query(params)\n",
    "\n",
    "        print(f\"ðŸ“Š Extracted Parameters: {params}\")\n",
    "        print(f\"ðŸ”¥ Executing Flux Query:\\n{flux_query}\") \n",
    "\n",
    "        # Execute the query using InfluxDB client\n",
    "        result = query_api.query(org=INFLUXDB_ORG, query=flux_query)\n",
    "        results = []\n",
    "\n",
    "        # Format the results\n",
    "        for table in result:\n",
    "            for record in table.records:\n",
    "                results.append(f\"Time: {record.get_time()}, Value: {record.get_value()}\")\n",
    "\n",
    "        return \"\\n\".join(results) if results else \"âš ï¸ No data found in the database. Verify if data exists for this time range.\"\n",
    "    \n",
    "    except ValueError as ve:\n",
    "        return str(ve)  # Return validation error messages\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error querying InfluxDB: {str(e)}\"\n",
    "\n",
    "# Function to extract the time range from a query\n",
    "def extract_time_range(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the time range from a user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The input query from the user.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted time range for InfluxDB (e.g., \"24h\", \"7d\", \"30d\").\n",
    "    \"\"\"\n",
    "    time_patterns = {\n",
    "        r\"(\\d+)\\s*(minute|minutes|min)\": \"m\",\n",
    "        r\"(\\d+)\\s*(hour|hours|h)\": \"h\",\n",
    "        r\"(\\d+)\\s*(day|days|d)\": \"d\",\n",
    "        r\"(\\d+)\\s*(week|weeks|w)\": \"w\",\n",
    "        r\"(\\d+)\\s*(month|months|mo)\": \"d\",  # Approximate: 1 month = 30 days\n",
    "        r\"(\\d+)\\s*(year|years|y)\": \"d\"  # Approximate: 1 year = 365 days\n",
    "    }\n",
    "\n",
    "    detected_range = \"24h\"  # Default if no time range is found\n",
    "\n",
    "    for pattern, unit in time_patterns.items():\n",
    "        match = re.search(pattern, user_query, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = int(match.group(1))  # Extract numeric value\n",
    "            if \"month\" in pattern:\n",
    "                value *= 30  # Convert months to days\n",
    "            elif \"year\" in pattern:\n",
    "                value *= 365  # Convert years to days\n",
    "            detected_range = f\"{value}{unit}\"\n",
    "            break\n",
    "\n",
    "    return detected_range\n",
    "\n",
    "# LangChain compatible tool\n",
    "\n",
    "influx_tool = StructuredTool.from_function(\n",
    "    name=\"InfluxDB Query Tool\",\n",
    "    description=(\n",
    "        \"Fetches sensor data from InfluxDB. \"\n",
    "        \"Requires parameters like metric (e.g., humidity, temperature), \"\n",
    "        \"time_range (e.g., 24h), and aggregation (e.g., mean).\"\n",
    "    ),\n",
    "    func=query_influxdb,\n",
    "    args_schema=InfluxDBQueryInput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Extracted Parameters: {'metric': 'temperature', 'time_range': '18d', 'aggregation': 'mean'}\n",
      "ðŸ”¥ Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -18d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"temperature\")\n",
      "      |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 14.9\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 14.91666666666667\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 14.912500000000001\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 14.863636363636367\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 14.774999999999997\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 14.733333333333333\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 14.688888888888888\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 14.633333333333331\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 14.618181818181817\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 14.599999999999998\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 14.559999999999999\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 14.430000000000001\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 14.400000000000002\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 14.363636363636367\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 14.350000000000003\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 14.350000000000001\n"
     ]
    }
   ],
   "source": [
    "result = query_influxdb(metric=\"temperature\", time_range=\"18d\", aggregation=\"mean\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elsevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the content of a specific article using Elsevier's APIs based on its title.\n",
    "\n",
    "    Args:\n",
    "    - title (str): The title of the article to search for.\n",
    "\n",
    "    Returns:\n",
    "    - str: The content or abstract of the specified article.\n",
    "    \"\"\"\n",
    "\n",
    "    API_KEY = \"87ab69edd16f0cdb92e611b99b8f4ee6\"\n",
    "    BASE_SEARCH_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    BASE_ARTICLE_URL = \"https://api.elsevier.com/content/article/doi\"\n",
    "    BASE_ARTICLE_EID_URL = \"https://api.elsevier.com/content/article/eid\"\n",
    "\n",
    "    # 1. Search article by title\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-ELS-APIKey\": API_KEY,\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": f\"TITLE({title})\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "    \n",
    "    search_response = requests.get(BASE_SEARCH_URL, headers=headers, params=params)\n",
    "\n",
    "    if search_response.status_code != 200:\n",
    "        return f\"Error: No se pudo buscar el artÃ­culo. CÃ³digo de estado: {search_response.status_code}, Error: {search_response.text}\"\n",
    "\n",
    "    search_data = search_response.json()\n",
    "    entries = search_data.get(\"search-results\", {}).get(\"entry\", [])\n",
    "\n",
    "    if not entries:\n",
    "        return f\"No se encontrÃ³ ningÃºn artÃ­culo con el tÃ­tulo '{title}'.\"\n",
    "\n",
    "    # 2. Fetch article identifier\n",
    "    article_entry = entries[0]\n",
    "    print(\"ArtÃ­culo encontrado:\", article_entry)  # Debugging info\n",
    "\n",
    "    doi = article_entry.get(\"prism:doi\")\n",
    "    eid = article_entry.get(\"eid\")\n",
    "\n",
    "    if not doi and not eid:\n",
    "        return f\"No se encontrÃ³ DOI ni EID para el artÃ­culo con el tÃ­tulo '{title}'. No se puede recuperar el contenido.\"\n",
    "\n",
    "    # 3. Fetch article content using DOI\n",
    "    if doi:\n",
    "        article_url = f\"{BASE_ARTICLE_URL}/{doi}\"\n",
    "    else:\n",
    "        article_url = f\"{BASE_ARTICLE_EID_URL}/{eid}\"\n",
    "\n",
    "    print(f\"Intentando recuperar el artÃ­culo desde: {article_url}\")  # Debugging info\n",
    "\n",
    "    article_response = requests.get(article_url, headers=headers)\n",
    "\n",
    "    if article_response.status_code != 200:\n",
    "        return f\"Error: No se pudo recuperar el contenido del artÃ­culo. CÃ³digo de estado: {article_response.status_code}, Error: {article_response.text}\"\n",
    "\n",
    "    article_data = article_response.json()\n",
    "\n",
    "    abstract = article_data.get(\"full-text-retrieval-response\", {}).get(\"coredata\", {}).get(\"dc:description\", \"No abstract found\")\n",
    "\n",
    "    return f\"Contenido del artÃ­culo '{title}':\\n\\n{abstract}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Taking the next step with generative artificial intelligence: The transformative role of multimodal large language models in science education\"\n",
    "content = get_article_content(title)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossref(subject: str) -> Dict:\n",
    "    limit = 5\n",
    "    results = {}\n",
    "    cr = Crossref()\n",
    "    try:\n",
    "        result = cr.works(query=subject, limit=limit)\n",
    "        for i in range(0, limit-1):\n",
    "            title = result['message']['items'][i].get('title', ['No Title'])[0]\n",
    "            abstract = result['message']['items'][i].get('abstract', 'No abstract available')\n",
    "            results[title] = abstract\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "crossref_tool = Tool(\n",
    "    name=\"Crossref article search\",\n",
    "    func=crossref,\n",
    "    description=\"Use this to fetch article titles and abstracts based on a given subject.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Crossref()\n",
    "x = cr.works(query = \"ecology\")\n",
    "x['message']['items'][4]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = crossref(subject=\"computing\")\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agent = WeatherAgent()\n",
    "\n",
    "print(\"Registered tools:\", weather_agent.agent.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agent.run(\"Is it raining in Manchester?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered tools: [StructuredTool(name='InfluxDB Query Tool', description='Fetches sensor data from InfluxDB. Requires parameters like metric (e.g., humidity, temperature), time_range (e.g., 24h), and aggregation (e.g., mean).', args_schema=<class 'Tools.DBTool.InfluxDBQueryInput'>, func=<function query_influxdb at 0x7bd5f81a9940>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/home/amartinez/MARTINEZ_Alex_Llm-Agents_2024/src/tfg/Agents/BaseAgent.py:44: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  self.agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "DB_agent = DBAgent()\n",
    "\n",
    "print(\"Registered tools:\", DB_agent.agent.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Query being processed by agent:\n",
      "What was the max humidity of the last 20 days\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"InfluxDB Query Tool\",\n",
      "  \"action_input\": {\n",
      "    \"metric\": \"humidity\",\n",
      "    \"time_range\": \"20d\",\n",
      "    \"aggregation\": \"max\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0mðŸ“Š Extracted Parameters: {'metric': 'humidity', 'time_range': '20d', 'aggregation': 'max'}\n",
      "ðŸ”¥ Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -20d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"humidity\")\n",
      "      |> aggregateWindow(every: 1h, fn: max, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTime: 2025-03-20 18:00:00+00:00, Value: 52.0\n",
      "Time: 2025-03-20 19:00:00+00:00, Value: 55.0\n",
      "Time: 2025-03-20 20:00:00+00:00, Value: 58.0\n",
      "Time: 2025-03-20 21:00:00+00:00, Value: 60.0\n",
      "Time: 2025-03-20 22:00:00+00:00, Value: 61.0\n",
      "Time: 2025-03-20 23:00:00+00:00, Value: 61.0\n",
      "Time: 2025-03-21 00:00:00+00:00, Value: 61.0\n",
      "Time: 2025-03-21 01:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 02:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 03:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 04:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 05:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 06:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 07:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 08:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 09:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 10:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 11:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 12:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 13:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 14:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 15:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 16:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 17:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 18:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 19:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 20:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 21:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-21 22:00:00+00:00, Value: 63.0\n",
      "Time: 2025-03-21 23:00:00+00:00, Value: 63.0\n",
      "Time: 2025-03-22 00:00:00+00:00, Value: 63.0\n",
      "Time: 2025-03-22 01:00:00+00:00, Value: 63.0\n",
      "Time: 2025-03-22 02:00:00+00:00, Value: 63.0\n",
      "Time: 2025-03-22 03:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 04:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 05:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 06:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 07:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 08:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 09:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 10:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 11:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 12:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 13:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 14:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 15:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 16:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 62.0\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 61.0\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 61.0\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 60.0\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 60.0\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 60.0\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 60.0\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 60.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The maximum humidity over the last 20 days was 63.0%\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The maximum humidity over the last 20 days was 63.0%'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_agent.run(\"What was the max humidity of the last 20 days\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elsevier Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsevier_agent = ElsevierAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Query being processed by agent:\n",
      "Search some articles about AI\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Elsevier Article Search\",\n",
      "  \"action_input\": \"Artificial Intelligence\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: Unable to retrieve the article content. Status code: 404, Error: {\"service-error\":{\"status\":{\"statusCode\":\"RESOURCE_NOT_FOUND\",\"statusText\":\"The resource specified cannot be found.\"}}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Elsevier Article Search\",\n",
      "  \"action_input\": \"Artificial Intelligence in Healthcare\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: Unable to retrieve the article content. Status code: 404, Error: {\"service-error\":{\"status\":{\"statusCode\":\"RESOURCE_NOT_FOUND\",\"statusText\":\"The resource specified cannot be found.\"}}}\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43melsevier_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSearch some articles about AI\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/sdb1/home/amartinez/MARTINEZ_Alex_Llm-Agents_2024/src/tfg/Agents/BaseAgent.py:63\u001b[39m, in \u001b[36mBaseAgent.run\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸš€ Query being processed by agent:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:188\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    187\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:606\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    605\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    607\u001b[39m         _output_key\n\u001b[32m    608\u001b[39m     ]\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    612\u001b[39m         _output_key\n\u001b[32m    613\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:188\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    187\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1624\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1622\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1632\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1633\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1330\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1323\u001b[39m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1327\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1328\u001b[39m ) -> Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1340\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1358\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1355\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1357\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:804\u001b[39m, in \u001b[36mAgent.plan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    792\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[32m    793\u001b[39m \n\u001b[32m    794\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    801\u001b[39m \u001b[33;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    803\u001b[39m full_inputs = \u001b[38;5;28mself\u001b[39m.get_full_inputs(intermediate_steps, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m full_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_parser.parse(full_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:318\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m    306\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:188\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    187\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:126\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    123\u001b[39m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    124\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:138\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    146\u001b[39m         cast(List, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    147\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:764\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    761\u001b[39m     **kwargs: Any,\n\u001b[32m    762\u001b[39m ) -> LLMResult:\n\u001b[32m    763\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:971\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     run_managers = [\n\u001b[32m    958\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    959\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    979\u001b[39m     run_managers = [\n\u001b[32m    980\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    981\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    989\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:790\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    780\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    781\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     **kwargs: Any,\n\u001b[32m    787\u001b[39m ) -> LLMResult:\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    789\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    798\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    799\u001b[39m         )\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    801\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_google_vertexai/llms.py:252\u001b[39m, in \u001b[36mVertexAI._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m     generations.append([generation])\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     res = \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshould_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_is_gemini_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_gemini_model:\n\u001b[32m    261\u001b[39m         usage_metadata = res.to_dict().get(\u001b[33m\"\u001b[39m\u001b[33musage_metadata\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/langchain_google_vertexai/llms.py:71\u001b[39m, in \u001b[36m_completion_with_retry\u001b[39m\u001b[34m(llm, prompt, stream, is_gemini, run_manager, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m llm.client.predict(prompt[\u001b[32m0\u001b[39m], **kwargs)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m telemetry.tool_context_manager(llm._user_agent):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tenacity/__init__.py:487\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    486\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MARTINEZ_Alex_Llm-Agents_2024/.venv/lib/python3.12/site-packages/tenacity/nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "elsevier_agent.run(\"Search some articles about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossref Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_agent = CrossrefAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_agent.run(\"I want to know more about quantum computing, can you help me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents dictionary\n",
    "agents = {\n",
    "    \"weather\": weather_agent,\n",
    "    \"article title\": crossref_agent,\n",
    "    \"article content\": elsevier_agent,\n",
    "    \"generic\": weather_agent,  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the router\n",
    "router = CustomAgentRouter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is the weather like in Madrid?\",  # Should go to WeatherAgent\n",
    "    \"Search for articles about machine learning.\",  # Should go to CrossrefAgent\n",
    "    \"I need the content of an article titled 'Deep Learning Basics'.\"  # Should go to ElsevierAgent\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline(\"What is the weather like in Madrid?\")\n",
    "print(\"Predictions output:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = router.run( \"What is the weather like in Madrid?\")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in inputs:\n",
    "    print(f\"Input: {input_text}\")\n",
    "    response = router.run(input_text)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Dict, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from CustomAgentRouter import CustomAgentRouter\n",
    "from Agents.WeatherAgent import WeatherAgent\n",
    "from Agents.DBAgent import DBAgent\n",
    "from Agents.CrossrefAgent import CrossrefAgent\n",
    "from Agents.ElsevierAgent import ElsevierAgent\n",
    "import transformers\n",
    "from langchain.schema import HumanMessage\n",
    "transformers.logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Define the state structure with memory\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    route: str\n",
    "    memory: ConversationBufferMemory\n",
    "\n",
    "def classify_query(state: State) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Uses CustomAgentRouter to classify user input and determine the correct agent.\n",
    "    \"\"\"\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    router = CustomAgentRouter()\n",
    "    category, _ = router.classify_text(user_input)\n",
    "    return {\"route\": category}\n",
    "\n",
    "def route_decision(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next node based on classification result.\n",
    "    \"\"\"\n",
    "    valid_routes = {\"weather\", \"database\", \"article title\", \"article content\"}\n",
    "    return state[\"route\"] if state[\"route\"] in valid_routes else \"generic\"\n",
    "\n",
    "# Agent functions with memory integration\n",
    "def weather_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = WeatherAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "def db_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = DBAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "def crossref_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = CrossrefAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "def elsevier_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = ElsevierAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "# Define the LangGraph workflow\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"classify\", classify_query)\n",
    "graph.add_node(\"weather\", weather_agent)\n",
    "graph.add_node(\"database\", db_agent)\n",
    "graph.add_node(\"crossref\", crossref_agent)\n",
    "graph.add_node(\"elsevier\", elsevier_agent)\n",
    "graph.add_node(\"generic\", lambda state: {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": \"I'm not sure how to help with that.\"}]})\n",
    "\n",
    "graph.add_edge(START, \"classify\")\n",
    "graph.add_conditional_edges(\"classify\", route_decision, {\n",
    "    \"weather\": \"weather\",\n",
    "    \"database\": \"database\",\n",
    "    \"article title\": \"crossref\",\n",
    "    \"article content\": \"elsevier\",\n",
    "    \"generic\": \"generic\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"weather\", END)\n",
    "graph.add_edge(\"database\", END)\n",
    "graph.add_edge(\"crossref\", END)\n",
    "graph.add_edge(\"elsevier\", END)\n",
    "graph.add_edge(\"generic\", END)\n",
    "\n",
    "compiled_graph = graph.compile()\n",
    "\n",
    "def run_conversation(user_input: str):\n",
    "    state = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
    "        \"memory\": ConversationBufferMemory()\n",
    "    }\n",
    "    for event in compiled_graph.stream(state):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                print(\"Assistant:\", value[\"messages\"][-1][\"content\"])\n",
    "            else:\n",
    "                print(\"Assistant: (No response received)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
