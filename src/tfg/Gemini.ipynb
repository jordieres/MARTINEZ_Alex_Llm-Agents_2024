{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from vertexai.generative_models import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import requests\n",
    "from typing import Dict\n",
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
    "from pyowm import OWM\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from langchain.tools import tool\n",
    "from habanero import Crossref\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_google_vertexai import VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agents.WeatherAgent import WeatherAgent\n",
    "from Agents.CrossrefAgent import CrossrefAgent\n",
    "from Agents.ElsevierAgent import ElsevierAgent\n",
    "from Agents.DBAgent import DBAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the memory usage\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API keys for various tools\n",
    "\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = \"54b1d6c1af466db06bee6219ddb33f7c\"\n",
    "INFLUXDB_URL = \"https://apiivm78.etsii.upm.es:8086\"\n",
    "INFLUXDB_TOKEN = \"bYNCMsvuiCEoJfPFL5gPgWgDISh79wO4dH9dF_y6cvOKp6uWTRZHtPIwEbRVb2gfFqo3AdygZCQIdbAGBfd31Q==\"\n",
    "INFLUXDB_ORG = \"UPM\"\n",
    "INFLUXDB_BUCKET = \"LoraWAN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"summer-surface-443821-r9\", location=\"europe-southwest1\")\n",
    "\n",
    "model = \"gemini-1.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings ={\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    # temperature (float): The sampling temperature controls the degree of\n",
    "    # randomness in token selection.\n",
    "    \"temperature\": 0.28,\n",
    "    # max_output_tokens (int): The token limit determines the maximum amount of\n",
    "    # text output from one prompt.\n",
    "    \"max_output_tokens\": 1000,\n",
    "    # top_p (float): Tokens are selected from most probable to least until\n",
    "    # the sum of their probabilities equals the top-p value.\n",
    "    \"top_p\": 0.95,\n",
    "    # top_k (int): The next token is selected from among the top-k most\n",
    "    # probable tokens.\n",
    "    \"top_k\": 40,\n",
    "    # safety_settings (Dict[HarmCategory, HarmBlockThreshold]): The safety\n",
    "    # settings to use for generating content.\n",
    "    \"safety_settings\": safety_settings,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3930683/2280263513.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Define memory buffer\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the answer structure\n",
    "\n",
    "PREFIX = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "FORMAT_INSTRUCTIONS = \"\"\"To use a tool, please use the following format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "```\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "```\"\"\"\n",
    "SUFFIX = \"\"\"Begin!\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "\n",
    "template = \"\\n\\n\".join([PREFIX, \"{tools}\", FORMAT_INSTRUCTIONS, SUFFIX])\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\", \"chat_history\", \"tools\", \"tool_names\", \"agent_scratchpad\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenWeatherMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Define expected input schema\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str\n",
    "    date: str | None = None  # Optional, in \"YYYY-MM-DD\" format\n",
    "\n",
    "def resolve_location(location: str) -> tuple[float, float] | None:\n",
    "    \"\"\"\n",
    "    Resolves a location name into latitude and longitude using Open-Meteo's geocoding API.\n",
    "\n",
    "    Args:\n",
    "        location (str): The name of the location (e.g., \"Madrid\" or \"New York\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing latitude and longitude as floats.\n",
    "               Returns None if the location is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"https://geocoding-api.open-meteo.com/v1/search\",\n",
    "            params={\"name\": location, \"count\": 1}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if \"results\" in data and len(data[\"results\"]) > 0:\n",
    "            result = data[\"results\"][0]\n",
    "            return result[\"latitude\"], result[\"longitude\"]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error resolving location: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_weather(location: str, date: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Fetches current or historical weather data for a given location using Open-Meteo.\n",
    "\n",
    "    Args:\n",
    "        location (str): The name of the location (e.g., \"Madrid\" or \"New York\").\n",
    "        date (str, optional): A historical date in \"YYYY-MM-DD\" format. If omitted, current weather is returned.\n",
    "\n",
    "    Returns:\n",
    "        str: A string describing the weather conditions for the given location and date.\n",
    "             If an error occurs or the location is not found, returns an explanatory message.\n",
    "    \"\"\"\n",
    "    coords = resolve_location(location)\n",
    "    if coords is None:\n",
    "        return f\"Could not resolve the location: {location}.\"\n",
    "\n",
    "    lat, lon = coords\n",
    "\n",
    "    if date:\n",
    "        # Historical weather\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                \"https://archive-api.open-meteo.com/v1/archive\",\n",
    "                params={\n",
    "                    \"latitude\": lat,\n",
    "                    \"longitude\": lon,\n",
    "                    \"start_date\": date,\n",
    "                    \"end_date\": date,\n",
    "                    \"hourly\": \"temperature_2m,relative_humidity_2m\",\n",
    "                    \"timezone\": \"auto\"\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            temps = data.get(\"hourly\", {}).get(\"temperature_2m\", [])\n",
    "            hums = data.get(\"hourly\", {}).get(\"relative_humidity_2m\", [])\n",
    "            if temps and hums:\n",
    "                avg_temp = sum(temps) / len(temps)\n",
    "                avg_hum = sum(hums) / len(hums)\n",
    "                return (\n",
    "                    f\"On {date}, the average temperature in {location} was {avg_temp:.1f}°C \"\n",
    "                    f\"and the average relative humidity was {avg_hum:.1f}%.\"\n",
    "                )\n",
    "            else:\n",
    "                return f\"No weather data available for {location} on {date}.\"\n",
    "        except Exception as e:\n",
    "            return f\"Failed to fetch historical weather data: {str(e)}\"\n",
    "    else:\n",
    "        # Current weather\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                \"https://api.open-meteo.com/v1/forecast\",\n",
    "                params={\n",
    "                    \"latitude\": lat,\n",
    "                    \"longitude\": lon,\n",
    "                    \"current\": \"temperature_2m,relative_humidity_2m\",\n",
    "                    \"timezone\": \"auto\"\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            current = data.get(\"current\", {})\n",
    "            temp = current.get(\"temperature_2m\")\n",
    "            hum = current.get(\"relative_humidity_2m\")\n",
    "            if temp is not None and hum is not None:\n",
    "                return (\n",
    "                    f\"The current temperature in {location} is {temp}°C with a \"\n",
    "                    f\"relative humidity of {hum}%.\"\n",
    "                )\n",
    "            else:\n",
    "                return f\"No current weather data available for {location}.\"\n",
    "        except Exception as e:\n",
    "            return f\"Failed to fetch current weather data: {str(e)}\"\n",
    "\n",
    "# Create the tool\n",
    "weather_tool = StructuredTool.from_function(\n",
    "    name=\"weather_tool\",\n",
    "    description=(\n",
    "        \"Fetches current or historical weather data for a specified location. \"\n",
    "        \"You can provide a date (YYYY-MM-DD) to retrieve past weather data, \"\n",
    "        \"or leave it empty to get current weather.\"\n",
    "    ),\n",
    "    func=get_weather,\n",
    "    args_schema=WeatherInput,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Paris is 22.2°C with a relative humidity of 59%.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_tool.invoke({\"location\": \"Paris\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient\n",
    "from langchain.tools import Tool\n",
    "import re\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from langchain.tools.base import StructuredTool\n",
    "\n",
    "# Client configuration\n",
    "INFLUXDB_URL = \"https://apiivm78.etsii.upm.es:8086\"\n",
    "INFLUXDB_TOKEN = \"bYNCMsvuiCEoJfPFL5gPgWgDISh79wO4dH9dF_y6cvOKp6uWTRZHtPIwEbRVb2gfFqo3AdygZCQIdbAGBfd31Q==\"\n",
    "INFLUXDB_ORG = \"UPM\"\n",
    "INFLUXDB_BUCKET = \"LoraWAN\"\n",
    "\n",
    "client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Supported parameters\n",
    "VALID_METRICS = {\"temperature\", \"humidity\", \"light\", \"motion\", \"vdd\"}\n",
    "VALID_AGGREGATIONS = {\"mean\", \"max\", \"min\", \"sum\"}\n",
    "\n",
    "# Define expected input schema for the tool using Pydantic\n",
    "class InfluxDBQueryInput(BaseModel):\n",
    "    \"\"\"\n",
    "    Defines the input parameters required for querying InfluxDB.\n",
    "    \"\"\"\n",
    "    metric: str          # Sensor metric to query (e.g., temperature, humidity)\n",
    "    time_range: str      # Time range for the query (e.g., 24h, 7d)\n",
    "    aggregation: str     # Aggregation function (e.g., mean, max)\n",
    "\n",
    "# Function to construct Flux query dynamically\n",
    "def construct_flux_query(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a Flux query based on extracted parameters.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): A dictionary containing 'metric', 'time_range', and 'aggregation'.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted Flux query.\n",
    "    \"\"\"\n",
    "    measurement = \"sensor_data\"  # Default measurement\n",
    "    field = params.get(\"metric\", \"humidity\")  # Default metric\n",
    "    time_range = params.get(\"time_range\", \"24h\")  # Default to 24h if missing\n",
    "    aggregation = params.get(\"aggregation\", \"mean\")  # Default to mean\n",
    "\n",
    "    # Validate metric\n",
    "    if field not in VALID_METRICS:\n",
    "        raise ValueError(f\"❌ Invalid metric '{field}'. Available metrics: {', '.join(VALID_METRICS)}\")\n",
    "\n",
    "    # Validate aggregation function\n",
    "    if aggregation not in VALID_AGGREGATIONS:\n",
    "        raise ValueError(f\"❌ Invalid aggregation '{aggregation}'. Available functions: {', '.join(VALID_AGGREGATIONS)}\")\n",
    "\n",
    "    # Construct Flux query\n",
    "    flux_query = f\"\"\"\n",
    "    from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "      |> range(start: -{time_range})\n",
    "      |> filter(fn: (r) => r[\"_measurement\"] == \"{measurement}\")\n",
    "      |> filter(fn: (r) => r[\"_field\"] == \"{field}\")\n",
    "      |> aggregateWindow(every: 1h, fn: {aggregation}, createEmpty: false)\n",
    "      |> yield(name: \"result\")\n",
    "    \"\"\"\n",
    "    return flux_query\n",
    "\n",
    "# Function that receives individual arguments (required by StructuredTool)\n",
    "def query_influxdb(metric: str, time_range: str, aggregation: str) -> str:\n",
    "    \"\"\"\n",
    "    StructuredTool-compatible function to query InfluxDB using individual parameters.\n",
    "\n",
    "    Args:\n",
    "        metric (str): Sensor metric to query (e.g., \"temperature\", \"humidity\").\n",
    "        time_range (str): Time range for the query (e.g., \"24h\", \"7d\").\n",
    "        aggregation (str): Aggregation function to apply (e.g., \"mean\", \"max\").\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted result or error message.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"metric\": metric,\n",
    "        \"time_range\": time_range,\n",
    "        \"aggregation\": aggregation\n",
    "    }\n",
    "    return _query_influxdb_internal(params)\n",
    "\n",
    "# Internal function to perform the actual query logic\n",
    "def _query_influxdb_internal(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs and executes a Flux query on InfluxDB from parameter dictionary.\n",
    "\n",
    "    Args:\n",
    "        params (dict): Dictionary containing 'metric', 'time_range', and 'aggregation'.\n",
    "\n",
    "    Returns:\n",
    "        str: Query results or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build the Flux query dynamically from parameters\n",
    "        flux_query = construct_flux_query(params)\n",
    "\n",
    "        print(f\"📊 Extracted Parameters: {params}\")\n",
    "        print(f\"🔥 Executing Flux Query:\\n{flux_query}\") \n",
    "\n",
    "        # Execute the query using InfluxDB client\n",
    "        result = query_api.query(org=INFLUXDB_ORG, query=flux_query)\n",
    "        results = []\n",
    "\n",
    "        # Format the results\n",
    "        for table in result:\n",
    "            for record in table.records:\n",
    "                results.append(f\"Time: {record.get_time()}, Value: {record.get_value()}\")\n",
    "\n",
    "        return \"\\n\".join(results) if results else \"⚠️ No data found in the database. Verify if data exists for this time range.\"\n",
    "    \n",
    "    except ValueError as ve:\n",
    "        return str(ve)  # Return validation error messages\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error querying InfluxDB: {str(e)}\"\n",
    "\n",
    "# Function to extract the time range from a query\n",
    "def extract_time_range(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the time range from a user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The input query from the user.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted time range for InfluxDB (e.g., \"24h\", \"7d\", \"30d\").\n",
    "    \"\"\"\n",
    "    time_patterns = {\n",
    "        r\"(\\d+)\\s*(minute|minutes|min)\": \"m\",\n",
    "        r\"(\\d+)\\s*(hour|hours|h)\": \"h\",\n",
    "        r\"(\\d+)\\s*(day|days|d)\": \"d\",\n",
    "        r\"(\\d+)\\s*(week|weeks|w)\": \"w\",\n",
    "        r\"(\\d+)\\s*(month|months|mo)\": \"d\",  # Approximate: 1 month = 30 days\n",
    "        r\"(\\d+)\\s*(year|years|y)\": \"d\"  # Approximate: 1 year = 365 days\n",
    "    }\n",
    "\n",
    "    detected_range = \"24h\"  # Default if no time range is found\n",
    "\n",
    "    for pattern, unit in time_patterns.items():\n",
    "        match = re.search(pattern, user_query, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = int(match.group(1))  # Extract numeric value\n",
    "            if \"month\" in pattern:\n",
    "                value *= 30  # Convert months to days\n",
    "            elif \"year\" in pattern:\n",
    "                value *= 365  # Convert years to days\n",
    "            detected_range = f\"{value}{unit}\"\n",
    "            break\n",
    "\n",
    "    return detected_range\n",
    "\n",
    "# LangChain compatible tool\n",
    "\n",
    "influx_tool = StructuredTool.from_function(\n",
    "    name=\"InfluxDB Query Tool\",\n",
    "    description=(\n",
    "        \"Fetches sensor data from InfluxDB. \"\n",
    "        \"Requires parameters like metric (e.g., humidity, temperature), \"\n",
    "        \"time_range (e.g., 24h), and aggregation (e.g., mean).\"\n",
    "    ),\n",
    "    func=query_influxdb,\n",
    "    args_schema=InfluxDBQueryInput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Extracted Parameters: {'metric': 'temperature', 'time_range': '100d', 'aggregation': 'mean'}\n",
      "🔥 Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -100d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"temperature\")\n",
      "      |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "Time: 2025-01-13 16:00:00+00:00, Value: 12.1\n",
      "Time: 2025-01-13 17:00:00+00:00, Value: 12.124999999999998\n",
      "Time: 2025-01-13 18:00:00+00:00, Value: 12.091666666666663\n",
      "Time: 2025-01-13 19:00:00+00:00, Value: 12.066666666666665\n",
      "Time: 2025-01-13 20:00:00+00:00, Value: 12.033333333333331\n",
      "Time: 2025-01-13 21:00:00+00:00, Value: 12.0\n",
      "Time: 2025-01-13 22:00:00+00:00, Value: 11.975\n",
      "Time: 2025-01-13 23:00:00+00:00, Value: 11.925000000000002\n",
      "Time: 2025-01-14 00:00:00+00:00, Value: 11.900000000000004\n",
      "Time: 2025-01-14 01:00:00+00:00, Value: 11.900000000000004\n",
      "Time: 2025-01-14 02:00:00+00:00, Value: 11.900000000000004\n",
      "Time: 2025-01-14 03:00:00+00:00, Value: 11.891666666666667\n",
      "Time: 2025-01-14 04:00:00+00:00, Value: 11.875000000000002\n",
      "Time: 2025-01-14 05:00:00+00:00, Value: 11.808333333333332\n",
      "Time: 2025-01-14 06:00:00+00:00, Value: 11.816666666666668\n",
      "Time: 2025-01-14 07:00:00+00:00, Value: 11.800000000000002\n",
      "Time: 2025-01-14 08:00:00+00:00, Value: 11.766666666666666\n",
      "Time: 2025-01-14 09:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-01-14 10:00:00+00:00, Value: 11.658333333333333\n",
      "Time: 2025-01-14 11:00:00+00:00, Value: 11.674999999999999\n",
      "Time: 2025-01-14 12:00:00+00:00, Value: 11.733333333333334\n",
      "Time: 2025-01-14 13:00:00+00:00, Value: 11.75\n",
      "Time: 2025-01-14 14:00:00+00:00, Value: 11.850000000000003\n",
      "Time: 2025-01-14 15:00:00+00:00, Value: 11.816666666666668\n",
      "Time: 2025-01-14 16:00:00+00:00, Value: 11.816666666666665\n",
      "Time: 2025-01-14 17:00:00+00:00, Value: 11.825000000000001\n",
      "Time: 2025-01-14 18:00:00+00:00, Value: 11.808333333333332\n",
      "Time: 2025-01-14 19:00:00+00:00, Value: 11.808333333333332\n",
      "Time: 2025-01-14 20:00:00+00:00, Value: 11.766666666666666\n",
      "Time: 2025-01-14 21:00:00+00:00, Value: 11.725\n",
      "Time: 2025-01-14 22:00:00+00:00, Value: 11.708333333333334\n",
      "Time: 2025-01-14 23:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-01-15 00:00:00+00:00, Value: 11.674999999999999\n",
      "Time: 2025-01-15 01:00:00+00:00, Value: 11.633333333333333\n",
      "Time: 2025-01-15 02:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-01-15 03:00:00+00:00, Value: 11.58333333333333\n",
      "Time: 2025-01-15 04:00:00+00:00, Value: 11.541666666666666\n",
      "Time: 2025-01-15 05:00:00+00:00, Value: 11.516666666666666\n",
      "Time: 2025-01-15 06:00:00+00:00, Value: 11.491666666666667\n",
      "Time: 2025-01-15 07:00:00+00:00, Value: 11.425000000000002\n",
      "Time: 2025-01-15 08:00:00+00:00, Value: 11.400000000000004\n",
      "Time: 2025-01-15 09:00:00+00:00, Value: 11.371428571428572\n",
      "Time: 2025-01-15 10:00:00+00:00, Value: 11.350000000000001\n",
      "Time: 2025-01-15 11:00:00+00:00, Value: 11.375000000000002\n",
      "Time: 2025-01-15 12:00:00+00:00, Value: 11.372727272727275\n",
      "Time: 2025-01-15 13:00:00+00:00, Value: 11.425000000000002\n",
      "Time: 2025-01-15 14:00:00+00:00, Value: 11.475\n",
      "Time: 2025-01-15 15:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-15 16:00:00+00:00, Value: 11.508333333333333\n",
      "Time: 2025-01-15 17:00:00+00:00, Value: 11.516666666666666\n",
      "Time: 2025-01-15 18:00:00+00:00, Value: 11.508333333333333\n",
      "Time: 2025-01-15 19:00:00+00:00, Value: 11.508333333333333\n",
      "Time: 2025-01-15 20:00:00+00:00, Value: 11.508333333333333\n",
      "Time: 2025-01-15 21:00:00+00:00, Value: 11.491666666666667\n",
      "Time: 2025-01-15 22:00:00+00:00, Value: 11.491666666666667\n",
      "Time: 2025-01-15 23:00:00+00:00, Value: 11.466666666666669\n",
      "Time: 2025-01-16 00:00:00+00:00, Value: 11.41666666666667\n",
      "Time: 2025-01-16 01:00:00+00:00, Value: 11.408333333333337\n",
      "Time: 2025-01-16 02:00:00+00:00, Value: 11.391666666666667\n",
      "Time: 2025-01-16 03:00:00+00:00, Value: 11.383333333333335\n",
      "Time: 2025-01-16 04:00:00+00:00, Value: 11.341666666666669\n",
      "Time: 2025-01-16 05:00:00+00:00, Value: 11.308333333333332\n",
      "Time: 2025-01-16 06:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-16 07:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-16 08:00:00+00:00, Value: 11.29090909090909\n",
      "Time: 2025-01-16 09:00:00+00:00, Value: 11.266666666666666\n",
      "Time: 2025-01-16 10:00:00+00:00, Value: 11.216666666666669\n",
      "Time: 2025-01-16 11:00:00+00:00, Value: 11.258333333333333\n",
      "Time: 2025-01-16 12:00:00+00:00, Value: 11.341666666666669\n",
      "Time: 2025-01-16 13:00:00+00:00, Value: 11.383333333333335\n",
      "Time: 2025-01-16 14:00:00+00:00, Value: 11.4\n",
      "Time: 2025-01-16 15:00:00+00:00, Value: 11.41666666666667\n",
      "Time: 2025-01-16 16:00:00+00:00, Value: 11.425000000000002\n",
      "Time: 2025-01-16 17:00:00+00:00, Value: 11.458333333333334\n",
      "Time: 2025-01-16 18:00:00+00:00, Value: 11.466666666666667\n",
      "Time: 2025-01-16 19:00:00+00:00, Value: 11.483333333333334\n",
      "Time: 2025-01-16 20:00:00+00:00, Value: 11.450000000000003\n",
      "Time: 2025-01-16 21:00:00+00:00, Value: 11.450000000000001\n",
      "Time: 2025-01-16 22:00:00+00:00, Value: 11.483333333333334\n",
      "Time: 2025-01-16 23:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-17 00:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-17 01:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-17 02:00:00+00:00, Value: 11.466666666666667\n",
      "Time: 2025-01-17 03:00:00+00:00, Value: 11.475000000000001\n",
      "Time: 2025-01-17 04:00:00+00:00, Value: 11.466666666666669\n",
      "Time: 2025-01-17 05:00:00+00:00, Value: 11.491666666666667\n",
      "Time: 2025-01-17 06:00:00+00:00, Value: 11.491666666666667\n",
      "Time: 2025-01-17 07:00:00+00:00, Value: 11.483333333333334\n",
      "Time: 2025-01-17 08:00:00+00:00, Value: 11.454545454545455\n",
      "Time: 2025-01-17 09:00:00+00:00, Value: 11.440000000000001\n",
      "Time: 2025-01-17 10:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-17 11:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-17 12:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-17 13:00:00+00:00, Value: 11.518181818181818\n",
      "Time: 2025-01-17 14:00:00+00:00, Value: 11.590909090909088\n",
      "Time: 2025-01-17 15:00:00+00:00, Value: 11.591666666666663\n",
      "Time: 2025-01-17 16:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-01-17 17:00:00+00:00, Value: 11.616666666666665\n",
      "Time: 2025-01-17 18:00:00+00:00, Value: 11.654545454545454\n",
      "Time: 2025-01-17 19:00:00+00:00, Value: 11.649999999999999\n",
      "Time: 2025-01-17 20:00:00+00:00, Value: 11.641666666666664\n",
      "Time: 2025-01-17 21:00:00+00:00, Value: 11.666666666666666\n",
      "Time: 2025-01-17 22:00:00+00:00, Value: 11.624999999999998\n",
      "Time: 2025-01-17 23:00:00+00:00, Value: 11.65\n",
      "Time: 2025-01-18 00:00:00+00:00, Value: 11.624999999999998\n",
      "Time: 2025-01-18 01:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-01-18 02:00:00+00:00, Value: 11.591666666666663\n",
      "Time: 2025-01-18 03:00:00+00:00, Value: 11.566666666666665\n",
      "Time: 2025-01-18 04:00:00+00:00, Value: 11.525\n",
      "Time: 2025-01-18 05:00:00+00:00, Value: 11.508333333333333\n",
      "Time: 2025-01-18 06:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-18 07:00:00+00:00, Value: 11.475000000000001\n",
      "Time: 2025-01-18 08:00:00+00:00, Value: 11.41666666666667\n",
      "Time: 2025-01-18 09:00:00+00:00, Value: 11.400000000000004\n",
      "Time: 2025-01-18 10:00:00+00:00, Value: 11.408333333333337\n",
      "Time: 2025-01-18 11:00:00+00:00, Value: 11.466666666666669\n",
      "Time: 2025-01-18 12:00:00+00:00, Value: 11.491666666666667\n",
      "Time: 2025-01-18 13:00:00+00:00, Value: 11.516666666666666\n",
      "Time: 2025-01-18 14:00:00+00:00, Value: 11.566666666666665\n",
      "Time: 2025-01-18 15:00:00+00:00, Value: 11.608333333333333\n",
      "Time: 2025-01-18 16:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-01-18 17:00:00+00:00, Value: 11.591666666666663\n",
      "Time: 2025-01-18 18:00:00+00:00, Value: 11.574999999999998\n",
      "Time: 2025-01-18 19:00:00+00:00, Value: 11.541666666666666\n",
      "Time: 2025-01-18 20:00:00+00:00, Value: 11.541666666666666\n",
      "Time: 2025-01-18 21:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-18 22:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-18 23:00:00+00:00, Value: 11.490909090909092\n",
      "Time: 2025-01-19 00:00:00+00:00, Value: 11.41666666666667\n",
      "Time: 2025-01-19 01:00:00+00:00, Value: 11.408333333333337\n",
      "Time: 2025-01-19 02:00:00+00:00, Value: 11.391666666666667\n",
      "Time: 2025-01-19 03:00:00+00:00, Value: 11.350000000000001\n",
      "Time: 2025-01-19 04:00:00+00:00, Value: 11.308333333333332\n",
      "Time: 2025-01-19 05:00:00+00:00, Value: 11.274999999999999\n",
      "Time: 2025-01-19 06:00:00+00:00, Value: 11.266666666666667\n",
      "Time: 2025-01-19 07:00:00+00:00, Value: 11.200000000000001\n",
      "Time: 2025-01-19 08:00:00+00:00, Value: 11.183333333333335\n",
      "Time: 2025-01-19 09:00:00+00:00, Value: 11.141666666666666\n",
      "Time: 2025-01-19 10:00:00+00:00, Value: 11.133333333333333\n",
      "Time: 2025-01-19 11:00:00+00:00, Value: 11.200000000000001\n",
      "Time: 2025-01-19 12:00:00+00:00, Value: 11.208333333333336\n",
      "Time: 2025-01-19 13:00:00+00:00, Value: 11.258333333333333\n",
      "Time: 2025-01-19 14:00:00+00:00, Value: 11.283333333333333\n",
      "Time: 2025-01-19 15:00:00+00:00, Value: 11.291666666666666\n",
      "Time: 2025-01-19 16:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-19 17:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-19 18:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-19 19:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-19 20:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-19 21:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-19 22:00:00+00:00, Value: 11.308333333333332\n",
      "Time: 2025-01-19 23:00:00+00:00, Value: 11.283333333333333\n",
      "Time: 2025-01-20 00:00:00+00:00, Value: 11.283333333333331\n",
      "Time: 2025-01-20 01:00:00+00:00, Value: 11.291666666666666\n",
      "Time: 2025-01-20 02:00:00+00:00, Value: 11.291666666666666\n",
      "Time: 2025-01-20 03:00:00+00:00, Value: 11.283333333333331\n",
      "Time: 2025-01-20 04:00:00+00:00, Value: 11.274999999999999\n",
      "Time: 2025-01-20 05:00:00+00:00, Value: 11.266666666666666\n",
      "Time: 2025-01-20 06:00:00+00:00, Value: 11.275\n",
      "Time: 2025-01-20 07:00:00+00:00, Value: 11.266666666666667\n",
      "Time: 2025-01-20 08:00:00+00:00, Value: 11.262500000000001\n",
      "Time: 2025-01-20 09:00:00+00:00, Value: 11.26\n",
      "Time: 2025-01-20 10:00:00+00:00, Value: 11.263636363636364\n",
      "Time: 2025-01-20 11:00:00+00:00, Value: 11.27142857142857\n",
      "Time: 2025-01-20 12:00:00+00:00, Value: 11.299999999999999\n",
      "Time: 2025-01-20 13:00:00+00:00, Value: 11.29090909090909\n",
      "Time: 2025-01-20 14:00:00+00:00, Value: 11.316666666666665\n",
      "Time: 2025-01-20 15:00:00+00:00, Value: 11.342857142857142\n",
      "Time: 2025-01-20 16:00:00+00:00, Value: 11.345454545454546\n",
      "Time: 2025-01-20 17:00:00+00:00, Value: 11.309999999999999\n",
      "Time: 2025-01-20 18:00:00+00:00, Value: 11.308333333333332\n",
      "Time: 2025-01-20 19:00:00+00:00, Value: 11.36\n",
      "Time: 2025-01-20 20:00:00+00:00, Value: 11.350000000000001\n",
      "Time: 2025-01-20 21:00:00+00:00, Value: 11.360000000000003\n",
      "Time: 2025-01-20 22:00:00+00:00, Value: 11.354545454545457\n",
      "Time: 2025-01-20 23:00:00+00:00, Value: 11.383333333333335\n",
      "Time: 2025-01-21 00:00:00+00:00, Value: 11.400000000000004\n",
      "Time: 2025-01-21 01:00:00+00:00, Value: 11.400000000000004\n",
      "Time: 2025-01-21 02:00:00+00:00, Value: 11.408333333333337\n",
      "Time: 2025-01-21 03:00:00+00:00, Value: 11.400000000000002\n",
      "Time: 2025-01-21 04:00:00+00:00, Value: 11.408333333333337\n",
      "Time: 2025-01-21 05:00:00+00:00, Value: 11.408333333333337\n",
      "Time: 2025-01-21 06:00:00+00:00, Value: 11.433333333333335\n",
      "Time: 2025-01-21 07:00:00+00:00, Value: 11.412500000000001\n",
      "Time: 2025-01-21 08:00:00+00:00, Value: 11.472727272727274\n",
      "Time: 2025-01-21 09:00:00+00:00, Value: 11.459999999999999\n",
      "Time: 2025-01-21 10:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-21 11:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-21 12:00:00+00:00, Value: 11.5\n",
      "Time: 2025-01-21 13:00:00+00:00, Value: 11.509090909090908\n",
      "Time: 2025-01-21 14:00:00+00:00, Value: 11.616666666666667\n",
      "Time: 2025-01-21 15:00:00+00:00, Value: 11.644444444444444\n",
      "Time: 2025-01-21 16:00:00+00:00, Value: 11.609999999999998\n",
      "Time: 2025-01-21 17:00:00+00:00, Value: 11.62\n",
      "Time: 2025-01-21 18:00:00+00:00, Value: 11.618181818181817\n",
      "Time: 2025-01-21 19:00:00+00:00, Value: 11.666666666666666\n",
      "Time: 2025-01-21 20:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-01-21 21:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-01-21 22:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-01-21 23:00:00+00:00, Value: 11.73\n",
      "Time: 2025-01-22 00:00:00+00:00, Value: 11.755555555555555\n",
      "Time: 2025-01-22 01:00:00+00:00, Value: 11.77272727272727\n",
      "Time: 2025-01-22 02:00:00+00:00, Value: 11.775\n",
      "Time: 2025-01-22 03:00:00+00:00, Value: 11.781818181818181\n",
      "Time: 2025-01-22 04:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-01-22 05:00:00+00:00, Value: 11.818181818181818\n",
      "Time: 2025-01-22 06:00:00+00:00, Value: 11.833333333333336\n",
      "Time: 2025-01-22 07:00:00+00:00, Value: 11.87777777777778\n",
      "Time: 2025-01-22 08:00:00+00:00, Value: 11.900000000000002\n",
      "Time: 2025-01-22 09:00:00+00:00, Value: 11.888888888888891\n",
      "Time: 2025-01-22 10:00:00+00:00, Value: 11.900000000000002\n",
      "Time: 2025-01-22 11:00:00+00:00, Value: 11.920000000000002\n",
      "Time: 2025-01-22 12:00:00+00:00, Value: 11.912500000000001\n",
      "Time: 2025-01-22 13:00:00+00:00, Value: 11.954545454545455\n",
      "Time: 2025-01-22 14:00:00+00:00, Value: 12.0\n",
      "Time: 2025-01-22 15:00:00+00:00, Value: 12.055555555555555\n",
      "Time: 2025-01-22 16:00:00+00:00, Value: 12.02\n",
      "Time: 2025-01-22 17:00:00+00:00, Value: 12.01\n",
      "Time: 2025-01-22 18:00:00+00:00, Value: 12.027272727272726\n",
      "Time: 2025-01-22 19:00:00+00:00, Value: 12.090909090909088\n",
      "Time: 2025-01-22 20:00:00+00:00, Value: 12.090909090909088\n",
      "Time: 2025-01-22 21:00:00+00:00, Value: 12.108333333333329\n",
      "Time: 2025-01-22 22:00:00+00:00, Value: 12.111111111111107\n",
      "Time: 2025-01-22 23:00:00+00:00, Value: 12.125\n",
      "Time: 2025-01-23 00:00:00+00:00, Value: 12.149999999999999\n",
      "Time: 2025-01-23 01:00:00+00:00, Value: 12.16\n",
      "Time: 2025-01-23 02:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-23 03:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-23 04:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-23 05:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-23 06:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-23 07:00:00+00:00, Value: 12.218181818181819\n",
      "Time: 2025-01-23 08:00:00+00:00, Value: 12.210000000000003\n",
      "Time: 2025-01-23 09:00:00+00:00, Value: 12.245454545454548\n",
      "Time: 2025-01-23 10:00:00+00:00, Value: 12.24\n",
      "Time: 2025-01-23 11:00:00+00:00, Value: 12.27\n",
      "Time: 2025-01-23 12:00:00+00:00, Value: 12.27142857142857\n",
      "Time: 2025-01-23 13:00:00+00:00, Value: 12.299999999999999\n",
      "Time: 2025-01-23 14:00:00+00:00, Value: 12.333333333333336\n",
      "Time: 2025-01-23 15:00:00+00:00, Value: 12.325000000000003\n",
      "Time: 2025-01-23 16:00:00+00:00, Value: 12.366666666666669\n",
      "Time: 2025-01-23 17:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-23 18:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-23 19:00:00+00:00, Value: 12.425000000000002\n",
      "Time: 2025-01-23 20:00:00+00:00, Value: 12.41666666666667\n",
      "Time: 2025-01-23 21:00:00+00:00, Value: 12.466666666666669\n",
      "Time: 2025-01-23 22:00:00+00:00, Value: 12.5\n",
      "Time: 2025-01-23 23:00:00+00:00, Value: 12.491666666666667\n",
      "Time: 2025-01-24 00:00:00+00:00, Value: 12.5\n",
      "Time: 2025-01-24 01:00:00+00:00, Value: 12.508333333333333\n",
      "Time: 2025-01-24 02:00:00+00:00, Value: 12.516666666666666\n",
      "Time: 2025-01-24 03:00:00+00:00, Value: 12.558333333333332\n",
      "Time: 2025-01-24 04:00:00+00:00, Value: 12.549999999999999\n",
      "Time: 2025-01-24 05:00:00+00:00, Value: 12.591666666666663\n",
      "Time: 2025-01-24 06:00:00+00:00, Value: 12.591666666666663\n",
      "Time: 2025-01-24 07:00:00+00:00, Value: 12.609090909090908\n",
      "Time: 2025-01-24 08:00:00+00:00, Value: 12.609999999999998\n",
      "Time: 2025-01-24 09:00:00+00:00, Value: 12.609090909090908\n",
      "Time: 2025-01-24 10:00:00+00:00, Value: 12.62\n",
      "Time: 2025-01-24 11:00:00+00:00, Value: 12.69090909090909\n",
      "Time: 2025-01-24 12:00:00+00:00, Value: 12.680000000000001\n",
      "Time: 2025-01-24 13:00:00+00:00, Value: 12.683333333333332\n",
      "Time: 2025-01-24 14:00:00+00:00, Value: 12.745454545454544\n",
      "Time: 2025-01-24 15:00:00+00:00, Value: 12.725000000000001\n",
      "Time: 2025-01-24 16:00:00+00:00, Value: 12.758333333333333\n",
      "Time: 2025-01-24 17:00:00+00:00, Value: 12.758333333333333\n",
      "Time: 2025-01-24 18:00:00+00:00, Value: 12.758333333333335\n",
      "Time: 2025-01-24 19:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-24 20:00:00+00:00, Value: 12.791666666666666\n",
      "Time: 2025-01-24 21:00:00+00:00, Value: 12.808333333333332\n",
      "Time: 2025-01-24 22:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-24 23:00:00+00:00, Value: 12.858333333333334\n",
      "Time: 2025-01-25 00:00:00+00:00, Value: 12.841666666666669\n",
      "Time: 2025-01-25 01:00:00+00:00, Value: 12.833333333333336\n",
      "Time: 2025-01-25 02:00:00+00:00, Value: 12.875000000000002\n",
      "Time: 2025-01-25 03:00:00+00:00, Value: 12.891666666666667\n",
      "Time: 2025-01-25 04:00:00+00:00, Value: 12.875000000000002\n",
      "Time: 2025-01-25 05:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-25 06:00:00+00:00, Value: 12.891666666666671\n",
      "Time: 2025-01-25 07:00:00+00:00, Value: 12.91666666666667\n",
      "Time: 2025-01-25 08:00:00+00:00, Value: 12.891666666666671\n",
      "Time: 2025-01-25 09:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-25 10:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-25 11:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-25 12:00:00+00:00, Value: 12.908333333333337\n",
      "Time: 2025-01-25 13:00:00+00:00, Value: 12.908333333333337\n",
      "Time: 2025-01-25 14:00:00+00:00, Value: 12.91666666666667\n",
      "Time: 2025-01-25 15:00:00+00:00, Value: 12.925000000000002\n",
      "Time: 2025-01-25 16:00:00+00:00, Value: 12.950000000000001\n",
      "Time: 2025-01-25 17:00:00+00:00, Value: 12.950000000000001\n",
      "Time: 2025-01-25 18:00:00+00:00, Value: 12.958333333333334\n",
      "Time: 2025-01-25 19:00:00+00:00, Value: 12.933333333333335\n",
      "Time: 2025-01-25 20:00:00+00:00, Value: 12.966666666666669\n",
      "Time: 2025-01-25 21:00:00+00:00, Value: 12.908333333333337\n",
      "Time: 2025-01-25 22:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-25 23:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-26 00:00:00+00:00, Value: 12.875000000000002\n",
      "Time: 2025-01-26 01:00:00+00:00, Value: 12.82727272727273\n",
      "Time: 2025-01-26 02:00:00+00:00, Value: 12.833333333333336\n",
      "Time: 2025-01-26 03:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 04:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 05:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 06:00:00+00:00, Value: 12.791666666666666\n",
      "Time: 2025-01-26 07:00:00+00:00, Value: 12.800000000000002\n",
      "Time: 2025-01-26 08:00:00+00:00, Value: 12.791666666666666\n",
      "Time: 2025-01-26 09:00:00+00:00, Value: 12.775\n",
      "Time: 2025-01-26 10:00:00+00:00, Value: 12.783333333333333\n",
      "Time: 2025-01-26 11:00:00+00:00, Value: 12.791666666666666\n",
      "Time: 2025-01-26 12:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 13:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 14:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 15:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 16:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 17:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 18:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 19:00:00+00:00, Value: 12.825000000000003\n",
      "Time: 2025-01-26 20:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 21:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 22:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-26 23:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-27 00:00:00+00:00, Value: 12.816666666666668\n",
      "Time: 2025-01-27 01:00:00+00:00, Value: 12.816666666666668\n",
      "Time: 2025-01-27 02:00:00+00:00, Value: 12.82727272727273\n",
      "Time: 2025-01-27 03:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-27 04:00:00+00:00, Value: 12.816666666666668\n",
      "Time: 2025-01-27 05:00:00+00:00, Value: 12.816666666666668\n",
      "Time: 2025-01-27 06:00:00+00:00, Value: 12.863636363636363\n",
      "Time: 2025-01-27 07:00:00+00:00, Value: 12.84\n",
      "Time: 2025-01-27 08:00:00+00:00, Value: 12.84\n",
      "Time: 2025-01-27 09:00:00+00:00, Value: 12.872727272727275\n",
      "Time: 2025-01-27 10:00:00+00:00, Value: 12.875000000000002\n",
      "Time: 2025-01-27 11:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-27 12:00:00+00:00, Value: 12.887500000000003\n",
      "Time: 2025-01-27 13:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-27 14:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-27 15:00:00+00:00, Value: 12.909090909090912\n",
      "Time: 2025-01-27 16:00:00+00:00, Value: 12.909090909090912\n",
      "Time: 2025-01-27 17:00:00+00:00, Value: 12.900000000000002\n",
      "Time: 2025-01-27 18:00:00+00:00, Value: 12.908333333333337\n",
      "Time: 2025-01-27 19:00:00+00:00, Value: 12.92727272727273\n",
      "Time: 2025-01-27 20:00:00+00:00, Value: 12.958333333333336\n",
      "Time: 2025-01-27 21:00:00+00:00, Value: 12.94166666666667\n",
      "Time: 2025-01-27 22:00:00+00:00, Value: 12.963636363636367\n",
      "Time: 2025-01-27 23:00:00+00:00, Value: 12.925000000000002\n",
      "Time: 2025-01-28 00:00:00+00:00, Value: 12.908333333333337\n",
      "Time: 2025-01-28 01:00:00+00:00, Value: 12.925000000000002\n",
      "Time: 2025-01-28 02:00:00+00:00, Value: 12.91818181818182\n",
      "Time: 2025-01-28 03:00:00+00:00, Value: 12.908333333333337\n",
      "Time: 2025-01-28 04:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-28 05:00:00+00:00, Value: 12.908333333333337\n",
      "Time: 2025-01-28 06:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-28 07:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-28 08:00:00+00:00, Value: 12.900000000000004\n",
      "Time: 2025-01-28 09:00:00+00:00, Value: 12.866666666666669\n",
      "Time: 2025-01-28 10:00:00+00:00, Value: 12.845454545454546\n",
      "Time: 2025-01-28 11:00:00+00:00, Value: 12.841666666666669\n",
      "Time: 2025-01-28 12:00:00+00:00, Value: 12.881818181818183\n",
      "Time: 2025-01-28 13:00:00+00:00, Value: 12.870000000000001\n",
      "Time: 2025-01-28 14:00:00+00:00, Value: 12.875000000000002\n",
      "Time: 2025-01-28 15:00:00+00:00, Value: 12.891666666666671\n",
      "Time: 2025-01-28 16:00:00+00:00, Value: 12.900000000000002\n",
      "Time: 2025-01-28 17:00:00+00:00, Value: 12.891666666666667\n",
      "Time: 2025-01-28 18:00:00+00:00, Value: 12.875000000000002\n",
      "Time: 2025-01-28 19:00:00+00:00, Value: 12.866666666666667\n",
      "Time: 2025-01-28 20:00:00+00:00, Value: 12.808333333333335\n",
      "Time: 2025-01-28 21:00:00+00:00, Value: 12.808333333333335\n",
      "Time: 2025-01-28 22:00:00+00:00, Value: 12.808333333333335\n",
      "Time: 2025-01-28 23:00:00+00:00, Value: 12.799999999999999\n",
      "Time: 2025-01-29 00:00:00+00:00, Value: 12.791666666666664\n",
      "Time: 2025-01-29 01:00:00+00:00, Value: 12.775\n",
      "Time: 2025-01-29 02:00:00+00:00, Value: 12.733333333333333\n",
      "Time: 2025-01-29 03:00:00+00:00, Value: 12.700000000000001\n",
      "Time: 2025-01-29 04:00:00+00:00, Value: 12.672727272727274\n",
      "Time: 2025-01-29 05:00:00+00:00, Value: 12.616666666666665\n",
      "Time: 2025-01-29 06:00:00+00:00, Value: 12.624999999999998\n",
      "Time: 2025-01-29 07:00:00+00:00, Value: 12.608333333333333\n",
      "Time: 2025-01-29 08:00:00+00:00, Value: 12.641666666666664\n",
      "Time: 2025-01-29 09:00:00+00:00, Value: 12.608333333333333\n",
      "Time: 2025-01-29 10:00:00+00:00, Value: 12.6\n",
      "Time: 2025-01-29 11:00:00+00:00, Value: 12.590909090909088\n",
      "Time: 2025-01-29 12:00:00+00:00, Value: 12.577777777777776\n",
      "Time: 2025-01-29 13:00:00+00:00, Value: 12.591666666666667\n",
      "Time: 2025-01-29 14:00:00+00:00, Value: 12.6625\n",
      "Time: 2025-01-29 15:00:00+00:00, Value: 12.616666666666665\n",
      "Time: 2025-01-29 16:00:00+00:00, Value: 12.599999999999998\n",
      "Time: 2025-01-29 17:00:00+00:00, Value: 12.599999999999996\n",
      "Time: 2025-01-29 18:00:00+00:00, Value: 12.608333333333333\n",
      "Time: 2025-01-29 19:00:00+00:00, Value: 12.599999999999996\n",
      "Time: 2025-01-29 20:00:00+00:00, Value: 12.591666666666663\n",
      "Time: 2025-01-29 21:00:00+00:00, Value: 12.566666666666665\n",
      "Time: 2025-01-29 22:00:00+00:00, Value: 12.533333333333331\n",
      "Time: 2025-01-29 23:00:00+00:00, Value: 12.5\n",
      "Time: 2025-01-30 00:00:00+00:00, Value: 12.5\n",
      "Time: 2025-01-30 01:00:00+00:00, Value: 12.5\n",
      "Time: 2025-01-30 02:00:00+00:00, Value: 12.5\n",
      "Time: 2025-01-30 03:00:00+00:00, Value: 12.483333333333334\n",
      "Time: 2025-01-30 04:00:00+00:00, Value: 12.41666666666667\n",
      "Time: 2025-01-30 05:00:00+00:00, Value: 12.41666666666667\n",
      "Time: 2025-01-30 06:00:00+00:00, Value: 12.408333333333337\n",
      "Time: 2025-01-30 07:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-30 08:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-30 09:00:00+00:00, Value: 12.400000000000002\n",
      "Time: 2025-01-30 10:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-30 11:00:00+00:00, Value: 12.408333333333337\n",
      "Time: 2025-01-30 12:00:00+00:00, Value: 12.42727272727273\n",
      "Time: 2025-01-30 13:00:00+00:00, Value: 12.408333333333337\n",
      "Time: 2025-01-30 14:00:00+00:00, Value: 12.463636363636367\n",
      "Time: 2025-01-30 15:00:00+00:00, Value: 12.441666666666668\n",
      "Time: 2025-01-30 16:00:00+00:00, Value: 12.433333333333335\n",
      "Time: 2025-01-30 17:00:00+00:00, Value: 12.454545454545455\n",
      "Time: 2025-01-30 18:00:00+00:00, Value: 12.490909090909092\n",
      "Time: 2025-01-30 19:00:00+00:00, Value: 12.450000000000001\n",
      "Time: 2025-01-30 20:00:00+00:00, Value: 12.441666666666668\n",
      "Time: 2025-01-30 21:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-30 22:00:00+00:00, Value: 12.391666666666671\n",
      "Time: 2025-01-30 23:00:00+00:00, Value: 12.380000000000003\n",
      "Time: 2025-01-31 00:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-31 01:00:00+00:00, Value: 12.391666666666667\n",
      "Time: 2025-01-31 02:00:00+00:00, Value: 12.400000000000004\n",
      "Time: 2025-01-31 03:00:00+00:00, Value: 12.383333333333338\n",
      "Time: 2025-01-31 04:00:00+00:00, Value: 12.316666666666668\n",
      "Time: 2025-01-31 05:00:00+00:00, Value: 12.283333333333331\n",
      "Time: 2025-01-31 06:00:00+00:00, Value: 12.225\n",
      "Time: 2025-01-31 07:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-31 08:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-31 09:00:00+00:00, Value: 12.166666666666664\n",
      "Time: 2025-01-31 10:00:00+00:00, Value: 12.149999999999999\n",
      "Time: 2025-01-31 11:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-31 12:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-31 13:00:00+00:00, Value: 12.225000000000001\n",
      "Time: 2025-01-31 14:00:00+00:00, Value: 12.225\n",
      "Time: 2025-01-31 15:00:00+00:00, Value: 12.266666666666666\n",
      "Time: 2025-01-31 16:00:00+00:00, Value: 12.291666666666666\n",
      "Time: 2025-01-31 17:00:00+00:00, Value: 12.291666666666666\n",
      "Time: 2025-01-31 18:00:00+00:00, Value: 12.299999999999999\n",
      "Time: 2025-01-31 19:00:00+00:00, Value: 12.241666666666665\n",
      "Time: 2025-01-31 20:00:00+00:00, Value: 12.225\n",
      "Time: 2025-01-31 21:00:00+00:00, Value: 12.208333333333334\n",
      "Time: 2025-01-31 22:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-01-31 23:00:00+00:00, Value: 12.200000000000001\n",
      "Time: 2025-02-01 00:00:00+00:00, Value: 12.174999999999999\n",
      "Time: 2025-02-01 01:00:00+00:00, Value: 12.141666666666664\n",
      "Time: 2025-02-01 02:00:00+00:00, Value: 12.108333333333333\n",
      "Time: 2025-02-01 03:00:00+00:00, Value: 12.099999999999996\n",
      "Time: 2025-02-01 04:00:00+00:00, Value: 12.108333333333329\n",
      "Time: 2025-02-01 05:00:00+00:00, Value: 12.091666666666663\n",
      "Time: 2025-02-01 06:00:00+00:00, Value: 12.074999999999998\n",
      "Time: 2025-02-01 07:00:00+00:00, Value: 12.025\n",
      "Time: 2025-02-01 08:00:00+00:00, Value: 12.008333333333333\n",
      "Time: 2025-02-01 09:00:00+00:00, Value: 11.991666666666667\n",
      "Time: 2025-02-01 10:00:00+00:00, Value: 12.008333333333333\n",
      "Time: 2025-02-01 11:00:00+00:00, Value: 12.008333333333333\n",
      "Time: 2025-02-01 12:00:00+00:00, Value: 12.074999999999998\n",
      "Time: 2025-02-01 13:00:00+00:00, Value: 12.099999999999996\n",
      "Time: 2025-02-01 14:00:00+00:00, Value: 12.099999999999996\n",
      "Time: 2025-02-01 15:00:00+00:00, Value: 12.099999999999996\n",
      "Time: 2025-02-01 16:00:00+00:00, Value: 12.108333333333333\n",
      "Time: 2025-02-01 17:00:00+00:00, Value: 12.141666666666664\n",
      "Time: 2025-02-01 18:00:00+00:00, Value: 12.108333333333329\n",
      "Time: 2025-02-01 19:00:00+00:00, Value: 12.116666666666665\n",
      "Time: 2025-02-01 20:00:00+00:00, Value: 12.091666666666663\n",
      "Time: 2025-02-01 21:00:00+00:00, Value: 12.058333333333332\n",
      "Time: 2025-02-01 22:00:00+00:00, Value: 12.049999999999999\n",
      "Time: 2025-02-01 23:00:00+00:00, Value: 12.049999999999999\n",
      "Time: 2025-02-02 00:00:00+00:00, Value: 12.008333333333333\n",
      "Time: 2025-02-02 01:00:00+00:00, Value: 11.966666666666669\n",
      "Time: 2025-02-02 02:00:00+00:00, Value: 11.941666666666668\n",
      "Time: 2025-02-02 03:00:00+00:00, Value: 11.941666666666668\n",
      "Time: 2025-02-02 04:00:00+00:00, Value: 11.891666666666671\n",
      "Time: 2025-02-02 05:00:00+00:00, Value: 11.883333333333335\n",
      "Time: 2025-02-02 06:00:00+00:00, Value: 11.833333333333336\n",
      "Time: 2025-02-02 07:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-02 08:00:00+00:00, Value: 11.791666666666666\n",
      "Time: 2025-02-02 09:00:00+00:00, Value: 11.75\n",
      "Time: 2025-02-02 10:00:00+00:00, Value: 11.725000000000001\n",
      "Time: 2025-02-02 11:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-02 12:00:00+00:00, Value: 11.816666666666668\n",
      "Time: 2025-02-02 13:00:00+00:00, Value: 11.833333333333336\n",
      "Time: 2025-02-02 14:00:00+00:00, Value: 11.891666666666667\n",
      "Time: 2025-02-02 15:00:00+00:00, Value: 11.883333333333335\n",
      "Time: 2025-02-02 16:00:00+00:00, Value: 11.900000000000004\n",
      "Time: 2025-02-02 17:00:00+00:00, Value: 11.891666666666671\n",
      "Time: 2025-02-02 18:00:00+00:00, Value: 11.883333333333335\n",
      "Time: 2025-02-02 19:00:00+00:00, Value: 11.883333333333335\n",
      "Time: 2025-02-02 20:00:00+00:00, Value: 11.883333333333338\n",
      "Time: 2025-02-02 21:00:00+00:00, Value: 11.900000000000004\n",
      "Time: 2025-02-02 22:00:00+00:00, Value: 11.891666666666667\n",
      "Time: 2025-02-02 23:00:00+00:00, Value: 11.841666666666669\n",
      "Time: 2025-02-03 00:00:00+00:00, Value: 11.850000000000001\n",
      "Time: 2025-02-03 01:00:00+00:00, Value: 11.850000000000001\n",
      "Time: 2025-02-03 02:00:00+00:00, Value: 11.818181818181818\n",
      "Time: 2025-02-03 03:00:00+00:00, Value: 11.808333333333332\n",
      "Time: 2025-02-03 04:00:00+00:00, Value: 11.825000000000001\n",
      "Time: 2025-02-03 05:00:00+00:00, Value: 11.816666666666668\n",
      "Time: 2025-02-03 06:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-03 07:00:00+00:00, Value: 11.816666666666668\n",
      "Time: 2025-02-03 08:00:00+00:00, Value: 11.80909090909091\n",
      "Time: 2025-02-03 09:00:00+00:00, Value: 11.777777777777779\n",
      "Time: 2025-02-03 10:00:00+00:00, Value: 11.791666666666666\n",
      "Time: 2025-02-03 11:00:00+00:00, Value: 11.774999999999999\n",
      "Time: 2025-02-03 12:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-03 13:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-03 14:00:00+00:00, Value: 11.833333333333334\n",
      "Time: 2025-02-03 15:00:00+00:00, Value: 11.875000000000002\n",
      "Time: 2025-02-03 16:00:00+00:00, Value: 11.836363636363638\n",
      "Time: 2025-02-03 17:00:00+00:00, Value: 11.850000000000001\n",
      "Time: 2025-02-03 18:00:00+00:00, Value: 11.841666666666667\n",
      "Time: 2025-02-03 19:00:00+00:00, Value: 11.816666666666668\n",
      "Time: 2025-02-03 20:00:00+00:00, Value: 11.808333333333332\n",
      "Time: 2025-02-03 21:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-03 22:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-03 23:00:00+00:00, Value: 11.799999999999999\n",
      "Time: 2025-02-04 00:00:00+00:00, Value: 11.783333333333333\n",
      "Time: 2025-02-04 01:00:00+00:00, Value: 11.716666666666669\n",
      "Time: 2025-02-04 02:00:00+00:00, Value: 11.708333333333334\n",
      "Time: 2025-02-04 03:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-02-04 04:00:00+00:00, Value: 11.691666666666665\n",
      "Time: 2025-02-04 05:00:00+00:00, Value: 11.691666666666668\n",
      "Time: 2025-02-04 06:00:00+00:00, Value: 11.649999999999999\n",
      "Time: 2025-02-04 07:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-02-04 08:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-02-04 09:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-02-04 10:00:00+00:00, Value: 11.58333333333333\n",
      "Time: 2025-02-04 11:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-02-04 12:00:00+00:00, Value: 11.649999999999999\n",
      "Time: 2025-02-04 13:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-02-04 14:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-02-04 15:00:00+00:00, Value: 11.708333333333334\n",
      "Time: 2025-02-04 16:00:00+00:00, Value: 11.725000000000001\n",
      "Time: 2025-02-04 17:00:00+00:00, Value: 11.733333333333334\n",
      "Time: 2025-02-04 18:00:00+00:00, Value: 11.791666666666666\n",
      "Time: 2025-02-04 19:00:00+00:00, Value: 11.774999999999999\n",
      "Time: 2025-02-04 20:00:00+00:00, Value: 11.75\n",
      "Time: 2025-02-04 21:00:00+00:00, Value: 11.766666666666666\n",
      "Time: 2025-02-04 22:00:00+00:00, Value: 11.741666666666665\n",
      "Time: 2025-02-04 23:00:00+00:00, Value: 11.741666666666665\n",
      "Time: 2025-02-05 00:00:00+00:00, Value: 11.733333333333334\n",
      "Time: 2025-02-05 01:00:00+00:00, Value: 11.708333333333334\n",
      "Time: 2025-02-05 02:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-02-05 03:00:00+00:00, Value: 11.683333333333332\n",
      "Time: 2025-02-05 04:00:00+00:00, Value: 11.649999999999999\n",
      "Time: 2025-02-05 05:00:00+00:00, Value: 11.658333333333331\n",
      "Time: 2025-02-05 06:00:00+00:00, Value: 11.608333333333333\n",
      "Time: 2025-02-05 07:00:00+00:00, Value: 11.590909090909088\n",
      "Time: 2025-02-05 08:00:00+00:00, Value: 11.58333333333333\n",
      "Time: 2025-02-05 09:00:00+00:00, Value: 11.55\n",
      "Time: 2025-02-05 10:00:00+00:00, Value: 11.524999999999999\n",
      "Time: 2025-02-05 11:00:00+00:00, Value: 11.566666666666665\n",
      "Time: 2025-02-05 12:00:00+00:00, Value: 11.609090909090908\n",
      "Time: 2025-02-05 13:00:00+00:00, Value: 11.674999999999999\n",
      "Time: 2025-02-05 14:00:00+00:00, Value: 11.69090909090909\n",
      "Time: 2025-02-05 15:00:00+00:00, Value: 11.733333333333334\n",
      "Time: 2025-02-05 16:00:00+00:00, Value: 11.741666666666667\n",
      "Time: 2025-02-05 17:00:00+00:00, Value: 11.766666666666667\n",
      "Time: 2025-02-05 18:00:00+00:00, Value: 11.783333333333333\n",
      "Time: 2025-02-05 19:00:00+00:00, Value: 11.758333333333335\n",
      "Time: 2025-02-05 20:00:00+00:00, Value: 11.766666666666667\n",
      "Time: 2025-02-05 21:00:00+00:00, Value: 11.741666666666665\n",
      "Time: 2025-02-05 22:00:00+00:00, Value: 11.783333333333333\n",
      "Time: 2025-02-05 23:00:00+00:00, Value: 11.758333333333333\n",
      "Time: 2025-02-06 00:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-02-06 01:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-02-06 02:00:00+00:00, Value: 11.700000000000001\n",
      "Time: 2025-02-06 03:00:00+00:00, Value: 11.658333333333331\n",
      "Time: 2025-02-06 04:00:00+00:00, Value: 11.624999999999998\n",
      "Time: 2025-02-06 05:00:00+00:00, Value: 11.616666666666665\n",
      "Time: 2025-02-06 06:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-02-06 07:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-02-06 08:00:00+00:00, Value: 11.599999999999996\n",
      "Time: 2025-02-06 09:00:00+00:00, Value: 11.5125\n",
      "Time: 2025-02-06 10:00:00+00:00, Value: 11.5\n",
      "Time: 2025-02-06 11:00:00+00:00, Value: 3.442857142857142\n",
      "Time: 2025-03-17 13:00:00+00:00, Value: 20.466666666666665\n",
      "Time: 2025-03-17 14:00:00+00:00, Value: 21.48888888888889\n",
      "Time: 2025-03-17 15:00:00+00:00, Value: 22.166666666666668\n",
      "Time: 2025-03-17 16:00:00+00:00, Value: 22.227272727272727\n",
      "Time: 2025-03-17 17:00:00+00:00, Value: 22.474999999999998\n",
      "Time: 2025-03-17 18:00:00+00:00, Value: 20.18181818181818\n",
      "Time: 2025-03-17 19:00:00+00:00, Value: 17.72222222222222\n",
      "Time: 2025-03-17 20:00:00+00:00, Value: 16.827272727272724\n",
      "Time: 2025-03-17 21:00:00+00:00, Value: 16.319999999999997\n",
      "Time: 2025-03-17 22:00:00+00:00, Value: 15.87142857142857\n",
      "Time: 2025-03-17 23:00:00+00:00, Value: 15.628571428571428\n",
      "Time: 2025-03-18 00:00:00+00:00, Value: 15.437500000000002\n",
      "Time: 2025-03-18 01:00:00+00:00, Value: 15.266666666666667\n",
      "Time: 2025-03-18 02:00:00+00:00, Value: 15.166666666666666\n",
      "Time: 2025-03-18 03:00:00+00:00, Value: 15.036363636363637\n",
      "Time: 2025-03-18 04:00:00+00:00, Value: 14.942857142857145\n",
      "Time: 2025-03-18 05:00:00+00:00, Value: 14.85\n",
      "Time: 2025-03-18 06:00:00+00:00, Value: 14.766666666666667\n",
      "Time: 2025-03-18 07:00:00+00:00, Value: 14.689999999999998\n",
      "Time: 2025-03-18 08:00:00+00:00, Value: 14.629999999999999\n",
      "Time: 2025-03-18 09:00:00+00:00, Value: 14.579999999999998\n",
      "Time: 2025-03-18 10:00:00+00:00, Value: 14.51\n",
      "Time: 2025-03-18 11:00:00+00:00, Value: 14.475000000000001\n",
      "Time: 2025-03-18 12:00:00+00:00, Value: 14.444444444444445\n",
      "Time: 2025-03-18 13:00:00+00:00, Value: 15.61111111111111\n",
      "Time: 2025-03-18 14:00:00+00:00, Value: 20.95\n",
      "Time: 2025-03-18 15:00:00+00:00, Value: 22.437499999999996\n",
      "Time: 2025-03-18 16:00:00+00:00, Value: 22.357142857142858\n",
      "Time: 2025-03-18 17:00:00+00:00, Value: 22.325000000000003\n",
      "Time: 2025-03-18 19:00:00+00:00, Value: 21.633333333333336\n",
      "Time: 2025-03-18 20:00:00+00:00, Value: 18.66\n",
      "Time: 2025-03-18 21:00:00+00:00, Value: 17.912499999999998\n",
      "Time: 2025-03-18 22:00:00+00:00, Value: 17.3\n",
      "Time: 2025-03-18 23:00:00+00:00, Value: 16.633333333333333\n",
      "Time: 2025-03-19 00:00:00+00:00, Value: 16.333333333333332\n",
      "Time: 2025-03-19 01:00:00+00:00, Value: 16.1\n",
      "Time: 2025-03-19 02:00:00+00:00, Value: 15.975\n",
      "Time: 2025-03-19 03:00:00+00:00, Value: 15.8\n",
      "Time: 2025-03-19 04:00:00+00:00, Value: 15.649999999999999\n",
      "Time: 2025-03-19 05:00:00+00:00, Value: 15.6\n",
      "Time: 2025-03-19 06:00:00+00:00, Value: 15.6\n",
      "Time: 2025-03-19 07:00:00+00:00, Value: 15.437500000000002\n",
      "Time: 2025-03-19 08:00:00+00:00, Value: 15.344444444444447\n",
      "Time: 2025-03-19 09:00:00+00:00, Value: 20.191666666666666\n",
      "Time: 2025-03-19 10:00:00+00:00, Value: 22.316666666666666\n",
      "Time: 2025-03-19 11:00:00+00:00, Value: 21.733333333333334\n",
      "Time: 2025-03-19 12:00:00+00:00, Value: 20.318181818181817\n",
      "Time: 2025-03-19 13:00:00+00:00, Value: 19.499999999999996\n",
      "Time: 2025-03-19 14:00:00+00:00, Value: 22.0375\n",
      "Time: 2025-03-19 15:00:00+00:00, Value: 23.76666666666667\n",
      "Time: 2025-03-19 16:00:00+00:00, Value: 22.09090909090909\n",
      "Time: 2025-03-19 17:00:00+00:00, Value: 19.7\n",
      "Time: 2025-03-19 18:00:00+00:00, Value: 18.455555555555552\n",
      "Time: 2025-03-19 19:00:00+00:00, Value: 17.9375\n",
      "Time: 2025-03-19 20:00:00+00:00, Value: 17.591666666666665\n",
      "Time: 2025-03-19 21:00:00+00:00, Value: 17.344444444444445\n",
      "Time: 2025-03-19 22:00:00+00:00, Value: 17.2\n",
      "Time: 2025-03-19 23:00:00+00:00, Value: 17.058333333333334\n",
      "Time: 2025-03-20 00:00:00+00:00, Value: 16.90909090909091\n",
      "Time: 2025-03-20 01:00:00+00:00, Value: 16.78181818181818\n",
      "Time: 2025-03-20 02:00:00+00:00, Value: 16.725\n",
      "Time: 2025-03-20 03:00:00+00:00, Value: 16.624999999999996\n",
      "Time: 2025-03-20 04:00:00+00:00, Value: 16.58\n",
      "Time: 2025-03-20 05:00:00+00:00, Value: 16.5\n",
      "Time: 2025-03-20 06:00:00+00:00, Value: 16.44166666666667\n",
      "Time: 2025-03-20 07:00:00+00:00, Value: 16.38888888888889\n",
      "Time: 2025-03-20 08:00:00+00:00, Value: 16.325000000000006\n",
      "Time: 2025-03-20 09:00:00+00:00, Value: 16.28\n",
      "Time: 2025-03-20 10:00:00+00:00, Value: 16.230769230769226\n",
      "Time: 2025-03-20 11:00:00+00:00, Value: 16.133333333333333\n",
      "Time: 2025-03-20 12:00:00+00:00, Value: 16.12222222222222\n",
      "Time: 2025-03-20 13:00:00+00:00, Value: 16.299999999999997\n",
      "Time: 2025-03-20 14:00:00+00:00, Value: 20.229999999999997\n",
      "Time: 2025-03-20 15:00:00+00:00, Value: 22.175\n",
      "Time: 2025-03-20 16:00:00+00:00, Value: 21.02\n",
      "Time: 2025-03-20 17:00:00+00:00, Value: 21.67\n",
      "Time: 2025-03-20 18:00:00+00:00, Value: 21.95\n",
      "Time: 2025-03-20 19:00:00+00:00, Value: 21.599999999999998\n",
      "Time: 2025-03-20 20:00:00+00:00, Value: 19.619999999999997\n",
      "Time: 2025-03-20 21:00:00+00:00, Value: 18.525000000000002\n",
      "Time: 2025-03-20 22:00:00+00:00, Value: 18.06\n",
      "Time: 2025-03-20 23:00:00+00:00, Value: 17.736363636363635\n",
      "Time: 2025-03-21 00:00:00+00:00, Value: 17.536363636363635\n",
      "Time: 2025-03-21 01:00:00+00:00, Value: 17.381818181818186\n",
      "Time: 2025-03-21 02:00:00+00:00, Value: 17.227272727272723\n",
      "Time: 2025-03-21 03:00:00+00:00, Value: 17.127272727272725\n",
      "Time: 2025-03-21 04:00:00+00:00, Value: 17.05\n",
      "Time: 2025-03-21 05:00:00+00:00, Value: 16.990909090909092\n",
      "Time: 2025-03-21 06:00:00+00:00, Value: 16.900000000000002\n",
      "Time: 2025-03-21 07:00:00+00:00, Value: 16.862499999999997\n",
      "Time: 2025-03-21 08:00:00+00:00, Value: 16.800000000000004\n",
      "Time: 2025-03-21 09:00:00+00:00, Value: 16.733333333333334\n",
      "Time: 2025-03-21 10:00:00+00:00, Value: 16.691666666666663\n",
      "Time: 2025-03-21 11:00:00+00:00, Value: 16.566666666666666\n",
      "Time: 2025-03-21 12:00:00+00:00, Value: 16.46\n",
      "Time: 2025-03-21 13:00:00+00:00, Value: 16.31818181818182\n",
      "Time: 2025-03-21 14:00:00+00:00, Value: 16.254545454545454\n",
      "Time: 2025-03-21 15:00:00+00:00, Value: 16.172727272727272\n",
      "Time: 2025-03-21 16:00:00+00:00, Value: 16.0875\n",
      "Time: 2025-03-21 17:00:00+00:00, Value: 16.0\n",
      "Time: 2025-03-21 18:00:00+00:00, Value: 15.960000000000003\n",
      "Time: 2025-03-21 19:00:00+00:00, Value: 15.900000000000002\n",
      "Time: 2025-03-21 20:00:00+00:00, Value: 15.83846153846154\n",
      "Time: 2025-03-21 21:00:00+00:00, Value: 15.799999999999999\n",
      "Time: 2025-03-21 22:00:00+00:00, Value: 15.749999999999996\n",
      "Time: 2025-03-21 23:00:00+00:00, Value: 15.716666666666663\n",
      "Time: 2025-03-22 00:00:00+00:00, Value: 15.66\n",
      "Time: 2025-03-22 01:00:00+00:00, Value: 15.599999999999996\n",
      "Time: 2025-03-22 02:00:00+00:00, Value: 15.574999999999998\n",
      "Time: 2025-03-22 03:00:00+00:00, Value: 15.51\n",
      "Time: 2025-03-22 04:00:00+00:00, Value: 15.470000000000002\n",
      "Time: 2025-03-22 05:00:00+00:00, Value: 15.400000000000004\n",
      "Time: 2025-03-22 06:00:00+00:00, Value: 15.377777777777778\n",
      "Time: 2025-03-22 07:00:00+00:00, Value: 15.299999999999999\n",
      "Time: 2025-03-22 08:00:00+00:00, Value: 15.300000000000002\n",
      "Time: 2025-03-22 09:00:00+00:00, Value: 15.254545454545452\n",
      "Time: 2025-03-22 10:00:00+00:00, Value: 15.199999999999998\n",
      "Time: 2025-03-22 11:00:00+00:00, Value: 15.169999999999998\n",
      "Time: 2025-03-22 12:00:00+00:00, Value: 15.111111111111107\n",
      "Time: 2025-03-22 13:00:00+00:00, Value: 15.099999999999998\n",
      "Time: 2025-03-22 14:00:00+00:00, Value: 15.054545454545455\n",
      "Time: 2025-03-22 15:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 16:00:00+00:00, Value: 14.98\n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 14.954545454545457\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 14.91666666666667\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 14.912500000000001\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 14.863636363636367\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 14.774999999999997\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 14.733333333333333\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 14.688888888888888\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 14.633333333333331\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 14.618181818181817\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 14.599999999999998\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 14.559999999999999\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 14.430000000000001\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 14.400000000000002\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 14.363636363636367\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 14.350000000000003\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 14.350000000000001\n"
     ]
    }
   ],
   "source": [
    "result = query_influxdb(metric=\"temperature\", time_range=\"100d\", aggregation=\"mean\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elsevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the content of a specific article using Elsevier's APIs based on its title.\n",
    "\n",
    "    Args:\n",
    "    - title (str): The title of the article to search for.\n",
    "\n",
    "    Returns:\n",
    "    - str: The content or abstract of the specified article.\n",
    "    \"\"\"\n",
    "\n",
    "    API_KEY = \"87ab69edd16f0cdb92e611b99b8f4ee6\"\n",
    "    BASE_SEARCH_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    BASE_ARTICLE_URL = \"https://api.elsevier.com/content/article/doi\"\n",
    "    BASE_ARTICLE_EID_URL = \"https://api.elsevier.com/content/article/eid\"\n",
    "\n",
    "    # 1. Search article by title\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-ELS-APIKey\": API_KEY,\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": f\"TITLE({title})\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "    \n",
    "    search_response = requests.get(BASE_SEARCH_URL, headers=headers, params=params)\n",
    "\n",
    "    if search_response.status_code != 200:\n",
    "        return f\"Error: No se pudo buscar el artículo. Código de estado: {search_response.status_code}, Error: {search_response.text}\"\n",
    "\n",
    "    search_data = search_response.json()\n",
    "    entries = search_data.get(\"search-results\", {}).get(\"entry\", [])\n",
    "\n",
    "    if not entries:\n",
    "        return f\"No se encontró ningún artículo con el título '{title}'.\"\n",
    "\n",
    "    # 2. Fetch article identifier\n",
    "    article_entry = entries[0]\n",
    "    print(\"Artículo encontrado:\", article_entry)  # Debugging info\n",
    "\n",
    "    doi = article_entry.get(\"prism:doi\")\n",
    "    eid = article_entry.get(\"eid\")\n",
    "\n",
    "    if not doi and not eid:\n",
    "        return f\"No se encontró DOI ni EID para el artículo con el título '{title}'. No se puede recuperar el contenido.\"\n",
    "\n",
    "    # 3. Fetch article content using DOI\n",
    "    if doi:\n",
    "        article_url = f\"{BASE_ARTICLE_URL}/{doi}\"\n",
    "    else:\n",
    "        article_url = f\"{BASE_ARTICLE_EID_URL}/{eid}\"\n",
    "\n",
    "    print(f\"Intentando recuperar el artículo desde: {article_url}\")  # Debugging info\n",
    "\n",
    "    article_response = requests.get(article_url, headers=headers)\n",
    "\n",
    "    if article_response.status_code != 200:\n",
    "        return f\"Error: No se pudo recuperar el contenido del artículo. Código de estado: {article_response.status_code}, Error: {article_response.text}\"\n",
    "\n",
    "    article_data = article_response.json()\n",
    "\n",
    "    abstract = article_data.get(\"full-text-retrieval-response\", {}).get(\"coredata\", {}).get(\"dc:description\", \"No abstract found\")\n",
    "\n",
    "    return f\"Contenido del artículo '{title}':\\n\\n{abstract}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Taking the next step with generative artificial intelligence: The transformative role of multimodal large language models in science education\"\n",
    "content = get_article_content(title)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossref(subject: str) -> Dict:\n",
    "    limit = 5\n",
    "    results = {}\n",
    "    cr = Crossref()\n",
    "    try:\n",
    "        result = cr.works(query=subject, limit=limit)\n",
    "        for i in range(0, limit-1):\n",
    "            title = result['message']['items'][i].get('title', ['No Title'])[0]\n",
    "            abstract = result['message']['items'][i].get('abstract', 'No abstract available')\n",
    "            results[title] = abstract\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "crossref_tool = Tool(\n",
    "    name=\"Crossref article search\",\n",
    "    func=crossref,\n",
    "    description=\"Use this to fetch article titles and abstracts based on a given subject.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Crossref()\n",
    "x = cr.works(query = \"ecology\")\n",
    "x['message']['items'][4]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = crossref(subject=\"computing\")\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agent = WeatherAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can get you the weather for Madrid on April 6, 2025, but I can't specify the time. Would you like the weather for the whole day?\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_agent.run(\"What was the weather like in MAdrid the 6th of april of 2025 at 11am\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/home/amartinez/MARTINEZ_Alex_Llm-Agents_2024/src/tfg/Agents/DBAgent.py:34: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationBufferMemory()\n",
      "/mnt/sdb1/home/amartinez/MARTINEZ_Alex_Llm-Agents_2024/src/tfg/Agents/DBAgent.py:29: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  self.agent_executor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from Agents.DBAgent import DBAgent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "agent = DBAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 db_agent received query: What is the max temperature of the last 42 days?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "  \"action\": \"InfluxDB Query Tool\",\n",
      "  \"action_input\": {\n",
      "    \"metric\": \"temperature\",\n",
      "    \"time_range\": \"42d\",\n",
      "    \"aggregation\": \"max\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m📊 Extracted Parameters: {'metric': 'temperature', 'time_range': '42d', 'aggregation': 'max'}\n",
      "🔥 Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -42d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"temperature\")\n",
      "      |> aggregateWindow(every: 1h, fn: max, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTime: 2025-03-22 15:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 16:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 14.9\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 14.6\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 14.6\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 14.4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe previous query returned a timeseries of temperature values. To find the maximum temperature over the last 42 days, I need to aggregate these values using the max function. I will use the InfluxDB Query Tool again, but this time I will specify the aggregation as \"max\".\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"InfluxDB Query Tool\",\n",
      "  \"action_input\": {\n",
      "    \"metric\": \"temperature\",\n",
      "    \"time_range\": \"42d\",\n",
      "    \"aggregation\": \"max\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m📊 Extracted Parameters: {'metric': 'temperature', 'time_range': '42d', 'aggregation': 'max'}\n",
      "🔥 Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -42d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"temperature\")\n",
      "      |> aggregateWindow(every: 1h, fn: max, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTime: 2025-03-22 15:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 16:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 14.9\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 14.6\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 14.6\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 14.4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe tool is returning a timeseries, not the maximum value. I need to ensure the query returns a single maximum value.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"InfluxDB Query Tool\",\n",
      "  \"action_input\": {\n",
      "    \"metric\": \"temperature\",\n",
      "    \"time_range\": \"42d\",\n",
      "    \"aggregation\": \"max\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m📊 Extracted Parameters: {'metric': 'temperature', 'time_range': '42d', 'aggregation': 'max'}\n",
      "🔥 Executing Flux Query:\n",
      "\n",
      "    from(bucket: \"LoraWAN\")\n",
      "      |> range(start: -42d)\n",
      "      |> filter(fn: (r) => r[\"_measurement\"] == \"sensor_data\")\n",
      "      |> filter(fn: (r) => r[\"_field\"] == \"temperature\")\n",
      "      |> aggregateWindow(every: 1h, fn: max, createEmpty: false)\n",
      "      |> yield(name: \"result\")\n",
      "    \n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTime: 2025-03-22 15:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 16:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 17:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 18:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 19:00:00+00:00, Value: 15.0\n",
      "Time: 2025-03-22 20:00:00+00:00, Value: 14.9\n",
      "Time: 2025-03-22 21:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 22:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-22 23:00:00+00:00, Value: 14.8\n",
      "Time: 2025-03-23 00:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 01:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 02:00:00+00:00, Value: 14.7\n",
      "Time: 2025-03-23 03:00:00+00:00, Value: 14.6\n",
      "Time: 2025-03-23 04:00:00+00:00, Value: 14.6\n",
      "Time: 2025-03-23 05:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 06:00:00+00:00, Value: 14.5\n",
      "Time: 2025-03-23 07:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 08:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 09:00:00+00:00, Value: 14.4\n",
      "Time: 2025-03-23 10:00:00+00:00, Value: 14.4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe tool is still returning a timeseries. It seems like the aggregation function is not working as expected. I will try to extract the maximum value from the timeseries manually. From the timeseries, the maximum value is 15.0.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The maximum temperature of the last 42 days is 15.0.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"What is the max temperature of the last 42 days?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DBAgent' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mWhat was the max humidity of the last 20 days\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[31mAttributeError\u001b[39m: 'DBAgent' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "agent.run(\"What was the max humidity of the last 20 days\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elsevier Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsevier_agent = ElsevierAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sorry, I cannot fulfill this request. I am unable to retrieve the article content. The article was not found.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elsevier_agent.run(\"Search the article AI Open\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossref Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_agent = CrossrefAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I found these articles on quantum computing:\\n\\n\"Quantum Computing I\" - Abstract: A remarkable application of quantum mechanical concepts of coherent superposition and quantum entanglement is a quantum computer which can solve certain problems at speeds unbelievably faster than the conventional computer. In this chapter, the basic principles and the conditions for the implementation of the quantum computer are introduced and the limitations imposed by the probabilistic nature of quantum mechanics and the inevitable decoherence phenomenon are discussed. Next the basic building blocks, the quantum logic gates, are introduced. These include the Hadamard, the CNOT, and the quantum phase gates. After these preliminaries, the implementation of the Deutsch algorithm, quantum teleportation, and quantum dense coding in terms of the quantum logic gates are discussed. It is also shown how the Bell states can be produced and measured using a sequence of quantum logic gates.\\n\\n\"Quantum Computing II\" - Abstract: This chapter deals with some of the most prominent successes of quantum computing. The most well-known quantum computing algorithm, Shor’s algorithm for factoring a number in its prime factors, is discussed in details. The key to Shor’s algorithm is the quantum Fourier transform that is explained with the help of simple examples. The role of quantum entanglement is also discussed. The next important quantum computing algorithm is Grover’s algorithm that helps in searching an item in an unsorted database. This algorithm is motivated by first discussing a quantum shell game in which a pea hidden under one of the four shells is found in one measurement with certainty each time. This amazing result is then generalized to an arbitrary number of objects and Grover’s algorithm.\\n\\n\"Application: Quantum Computing\" - Abstract: Is information physical?\\nComputers process information. Computer scientist and physicist Rolf Landauer was a proponent of the idea that information is an aspect of the physical world. He elaborated this as follows: Information is not a disembodied abstract entity; it is always tied to...\\n\\n\"Distributed Quantum Computing\\u2009–\\u2009Classical and Quantum\" - No abstract available\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossref_agent.run(\"I want to know more about quantum computing, can you help me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents dictionary\n",
    "agents = {\n",
    "    \"weather\": weather_agent,\n",
    "    \"article title\": crossref_agent,\n",
    "    \"article content\": elsevier_agent,\n",
    "    \"generic\": weather_agent,  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the router\n",
    "router = CustomAgentRouter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is the weather like in Madrid?\",  # Should go to WeatherAgent\n",
    "    \"Search for articles about machine learning.\",  # Should go to CrossrefAgent\n",
    "    \"I need the content of an article titled 'Deep Learning Basics'.\"  # Should go to ElsevierAgent\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline(\"What is the weather like in Madrid?\")\n",
    "print(\"Predictions output:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = router.run( \"What is the weather like in Madrid?\")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in inputs:\n",
    "    print(f\"Input: {input_text}\")\n",
    "    response = router.run(input_text)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Dict, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from CustomAgentRouter import CustomAgentRouter\n",
    "from Agents.WeatherAgent import WeatherAgent\n",
    "from Agents.DBAgent import DBAgent\n",
    "from Agents.CrossrefAgent import CrossrefAgent\n",
    "from Agents.ElsevierAgent import ElsevierAgent\n",
    "import transformers\n",
    "from langchain.schema import HumanMessage\n",
    "transformers.logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Define the state structure with memory\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    route: str\n",
    "    memory: ConversationBufferMemory\n",
    "\n",
    "def classify_query(state: State) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Uses CustomAgentRouter to classify user input and determine the correct agent.\n",
    "    \"\"\"\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    router = CustomAgentRouter()\n",
    "    category, _ = router.classify_text(user_input)\n",
    "    return {\"route\": category}\n",
    "\n",
    "def route_decision(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next node based on classification result.\n",
    "    \"\"\"\n",
    "    valid_routes = {\"weather\", \"database\", \"article title\", \"article content\"}\n",
    "    return state[\"route\"] if state[\"route\"] in valid_routes else \"generic\"\n",
    "\n",
    "# Agent functions with memory integration\n",
    "def weather_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = WeatherAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "def db_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = DBAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "def crossref_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = CrossrefAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "def elsevier_agent(state: State) -> Dict[str, str]:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    agent = ElsevierAgent()\n",
    "    response = agent.run(user_input)\n",
    "    state[\"memory\"].save_context({\"user_input\": user_input}, {\"response\": response})\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": response}]}\n",
    "\n",
    "# Define the LangGraph workflow\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"classify\", classify_query)\n",
    "graph.add_node(\"weather\", weather_agent)\n",
    "graph.add_node(\"database\", db_agent)\n",
    "graph.add_node(\"crossref\", crossref_agent)\n",
    "graph.add_node(\"elsevier\", elsevier_agent)\n",
    "graph.add_node(\"generic\", lambda state: {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": \"I'm not sure how to help with that.\"}]})\n",
    "\n",
    "graph.add_edge(START, \"classify\")\n",
    "graph.add_conditional_edges(\"classify\", route_decision, {\n",
    "    \"weather\": \"weather\",\n",
    "    \"database\": \"database\",\n",
    "    \"article title\": \"crossref\",\n",
    "    \"article content\": \"elsevier\",\n",
    "    \"generic\": \"generic\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"weather\", END)\n",
    "graph.add_edge(\"database\", END)\n",
    "graph.add_edge(\"crossref\", END)\n",
    "graph.add_edge(\"elsevier\", END)\n",
    "graph.add_edge(\"generic\", END)\n",
    "\n",
    "compiled_graph = graph.compile()\n",
    "\n",
    "def run_conversation(user_input: str):\n",
    "    state = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
    "        \"memory\": ConversationBufferMemory()\n",
    "    }\n",
    "    for event in compiled_graph.stream(state):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                print(\"Assistant:\", value[\"messages\"][-1][\"content\"])\n",
    "            else:\n",
    "                print(\"Assistant: (No response received)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
